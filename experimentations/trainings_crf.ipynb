{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sangkak AI Challenge: POS tasks\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "- **Author**: Elvis MBONING (NTeALan Research and Development Team)\n",
    "- **Session**: septembre 2023\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "In this notebook, we try to train differents models.\n",
    "\n",
    "We want to train these hypothesis for CRF_suite model:\n",
    "\n",
    "- impact of features normalization to the model classification\n",
    "- impact of features regulatization to the model classification\n",
    "- impact of choice of classification algorithm to the model classification\n",
    "- impact of data augmentation based on position (imbalence classes) to the model classification\n",
    "- impact of data augmentation based on features (imbalence classes) to the model classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "In this experiment, we want to build ML model based on Conditional Random Field (CRF). \n",
    "\n",
    "## 1- Data processing and analysis\n",
    "\n",
    "### 1.1. Loading data from Masakhane folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install python packages dependencies\n",
    "!pip3 install pandas python_crfsuite summarytools sklearn_crfsuite\n",
    "!pip3 install iteration_utilities matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Masakhane dataset from Github\n",
    "!git clone https://github.com/masakhane-io/masakhane-pos.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from sangkak_estimators import SangkakPosProjetReader, SangkakPosFeaturisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Read input sentences\n",
      "-> Augment input sentences with pos to pos algorithm\n",
      "-> Read input sentences\n",
      "-> Augment input sentences with pos to pos algorithm\n",
      "-> Read input sentences\n",
      "-> Augment input sentences with pos to pos algorithm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mwɔ̌ʼ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pfʉ́tə́</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nə́</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mwâsi</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>máp</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210453</th>\n",
       "      <td>11131</td>\n",
       "      <td>Nəmo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210454</th>\n",
       "      <td>11131</td>\n",
       "      <td>Ntamtə</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210455</th>\n",
       "      <td>11131</td>\n",
       "      <td>Guŋ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210456</th>\n",
       "      <td>11131</td>\n",
       "      <td>áá</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210457</th>\n",
       "      <td>11131</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210458 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id     word   tags\n",
       "0                 1    Mwɔ̌ʼ   NOUN\n",
       "1                 1  pfʉ́tə́   VERB\n",
       "2                 1      nə́    ADP\n",
       "3                 1   mwâsi   NOUN\n",
       "4                 1     máp    DET\n",
       "...             ...      ...    ...\n",
       "210453        11131     Nəmo   NOUN\n",
       "210454        11131   Ntamtə   NOUN\n",
       "210455        11131      Guŋ   NOUN\n",
       "210456        11131     áá    DET\n",
       "210457        11131        .  PUNCT\n",
       "\n",
       "[210458 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get path of test data \n",
    "language = 'bbj'\n",
    "bbj_pos_path   = Path(f'../data_source/masakhane-pos/data/{language}')\n",
    "train_data_path = bbj_pos_path / 'train.txt'\n",
    "dev_data_path = bbj_pos_path / 'dev.txt'\n",
    "test_data_path = bbj_pos_path / 'test.txt'\n",
    "\n",
    "# read data from source with sklearn estimator\n",
    "reader_estimator = SangkakPosProjetReader()\n",
    "list_train_data, pd_train_data = reader_estimator.fit(train_data_path).transform_analysis(augment=True)\n",
    "list_dev_data, pd_dev_data = reader_estimator.fit(dev_data_path).transform_analysis(augment=True)\n",
    "list_test_data, pd_test_data = reader_estimator.fit(test_data_path).transform_analysis(augment=True)\n",
    "\n",
    "pd_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Featurisation of input sentences\n",
      "-> Featurisation of input sentences\n",
      "-> Featurisation of input sentences\n",
      "-> Featurisation of input sentences\n",
      "-> Featurisation of input sentences\n",
      "-> Featurisation of input sentences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'Mwɔ̌ʼ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̌',\n",
       "  'word.normalized': 'Mwɔ̌ʼ',\n",
       "  'word.position': 0,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'mwɔ̌ʼ',\n",
       "  'word.start_with_capital': -1,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'Mw',\n",
       "  'word.root': '̌ʼ',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': -1,\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 1,\n",
       "  '-1:word': '',\n",
       "  '-1:word.position': -1,\n",
       "  '-1:word.letters': -1,\n",
       "  '-1:word.normalized': '',\n",
       "  '-1:word.start_with_capital': -1,\n",
       "  '-1:len(word-1)': -1,\n",
       "  '-1:word.lower()': '',\n",
       "  '-1:word.isdigit()': -1,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': '',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'pf',\n",
       "  '+1:word.root': '́tə́',\n",
       "  '+1:word': 'pfʉ́tə́',\n",
       "  '+1:word.position': 1,\n",
       "  '+1:word.letters': 'p f ʉ ́ t ə ́',\n",
       "  '+1:word.normalized': 'pfʉ́tə́',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 7,\n",
       "  '+1:word.lower()': 'pfʉ́tə́',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'pfʉ́tə́',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'pfʉ́tə́',\n",
       "  'word.position': 1,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'pfʉ́tə́',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'pf',\n",
       "  'word.root': '́tə́',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'p f ʉ ́ t ə ́',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'Mwɔ̌ʼ',\n",
       "  '-1:word.position': 0,\n",
       "  '-1:word.letters': 'M w ɔ ̌ ʼ',\n",
       "  '-1:word.normalized': 'Mwɔ̌ʼ',\n",
       "  '-1:word.start_with_capital': 1,\n",
       "  '-1:len(word-1)': 5,\n",
       "  '-1:word.lower()': 'mwɔ̌ʼ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 1,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'Mw',\n",
       "  '-1:word.root': '̌ʼ',\n",
       "  '+1:word.prefix': 'nə',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'nə́',\n",
       "  '+1:word.position': 2,\n",
       "  '+1:word.letters': 'n ə ́',\n",
       "  '+1:word.normalized': 'nə́',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'nə́',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'nə́',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'nə́',\n",
       "  'word.position': 2,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'nə́',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'nə',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'n ə ́',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'pfʉ́tə́',\n",
       "  '-1:word.position': 1,\n",
       "  '-1:word.letters': 'p f ʉ ́ t ə ́',\n",
       "  '-1:word.normalized': 'pfʉ́tə́',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 7,\n",
       "  '-1:word.lower()': 'pfʉ́tə́',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'pf',\n",
       "  '-1:word.root': '́tə́',\n",
       "  '+1:word.prefix': 'mw',\n",
       "  '+1:word.root': '̂si',\n",
       "  '+1:word': 'mwâsi',\n",
       "  '+1:word.position': 3,\n",
       "  '+1:word.letters': 'm w a ̂ s i',\n",
       "  '+1:word.normalized': 'mwâsi',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 6,\n",
       "  '+1:word.lower()': 'mwâsi',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'mwâsi',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̂',\n",
       "  'word.normalized': 'mwâsi',\n",
       "  'word.position': 3,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'mwâsi',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'mw',\n",
       "  'word.root': '̂si',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'm w a ̂ s i',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'nə́',\n",
       "  '-1:word.position': 2,\n",
       "  '-1:word.letters': 'n ə ́',\n",
       "  '-1:word.normalized': 'nə́',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'nə́',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'nə',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'ma',\n",
       "  '+1:word.root': 'p',\n",
       "  '+1:word': 'máp',\n",
       "  '+1:word.position': 4,\n",
       "  '+1:word.letters': 'm a ́ p',\n",
       "  '+1:word.normalized': 'máp',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 4,\n",
       "  '+1:word.lower()': 'máp',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'máp',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'máp',\n",
       "  'word.position': 4,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'máp',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'ma',\n",
       "  'word.root': 'p',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'm a ́ p',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'mwâsi',\n",
       "  '-1:word.position': 3,\n",
       "  '-1:word.letters': 'm w a ̂ s i',\n",
       "  '-1:word.normalized': 'mwâsi',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 6,\n",
       "  '-1:word.lower()': 'mwâsi',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'mw',\n",
       "  '-1:word.root': '̂si',\n",
       "  '+1:word.prefix': 'yə',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'yə́',\n",
       "  '+1:word.position': 5,\n",
       "  '+1:word.letters': 'y ə ́',\n",
       "  '+1:word.normalized': 'yə́',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'yə́',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'yə́',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'yə́',\n",
       "  'word.position': 5,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'yə́',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'yə',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'y ə ́',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'máp',\n",
       "  '-1:word.position': 4,\n",
       "  '-1:word.letters': 'm a ́ p',\n",
       "  '-1:word.normalized': 'máp',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 4,\n",
       "  '-1:word.lower()': 'máp',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'ma',\n",
       "  '-1:word.root': 'p',\n",
       "  '+1:word.prefix': 'cw',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'cwə',\n",
       "  '+1:word.position': 6,\n",
       "  '+1:word.letters': 'c w ə',\n",
       "  '+1:word.normalized': 'cwə',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'cwə',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'cwə',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'cwə',\n",
       "  'word.position': 6,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'cwə',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 0,\n",
       "  'word.prefix': 'cw',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'c w ə',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'yə́',\n",
       "  '-1:word.position': 5,\n",
       "  '-1:word.letters': 'y ə ́',\n",
       "  '-1:word.normalized': 'yə́',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'yə́',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'yə',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'Cy',\n",
       "  '+1:word.root': 'pɔ',\n",
       "  '+1:word': 'Cyəpɔ',\n",
       "  '+1:word.position': 7,\n",
       "  '+1:word.letters': 'C y ə p ɔ',\n",
       "  '+1:word.normalized': 'Cyəpɔ',\n",
       "  '+1:word.start_with_capital': 1,\n",
       "  '+1:len(word+1)': 5,\n",
       "  '+1:word.lower()': 'cyəpɔ',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'Cyəpɔ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'Cyəpɔ',\n",
       "  'word.position': 7,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'cyəpɔ',\n",
       "  'word.start_with_capital': 1,\n",
       "  'word.have_tone': 0,\n",
       "  'word.prefix': 'Cy',\n",
       "  'word.root': 'pɔ',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'C y ə p ɔ',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'cwə',\n",
       "  '-1:word.position': 6,\n",
       "  '-1:word.letters': 'c w ə',\n",
       "  '-1:word.normalized': 'cwə',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'cwə',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'cw',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'Si',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'Sǐ',\n",
       "  '+1:word.position': 8,\n",
       "  '+1:word.letters': 'S i ̌',\n",
       "  '+1:word.normalized': 'Sǐ',\n",
       "  '+1:word.start_with_capital': 1,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'sǐ',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'Sǐ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̌',\n",
       "  'word.normalized': 'Sǐ',\n",
       "  'word.position': 8,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'sǐ',\n",
       "  'word.start_with_capital': 1,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'Si',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'S i ̌',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'Cyəpɔ',\n",
       "  '-1:word.position': 7,\n",
       "  '-1:word.letters': 'C y ə p ɔ',\n",
       "  '-1:word.normalized': 'Cyəpɔ',\n",
       "  '-1:word.start_with_capital': 1,\n",
       "  '-1:len(word-1)': 5,\n",
       "  '-1:word.lower()': 'cyəpɔ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'Cy',\n",
       "  '-1:word.root': 'pɔ',\n",
       "  '+1:word.prefix': 'ku',\n",
       "  '+1:word.root': 'ʼ',\n",
       "  '+1:word': 'kùʼ',\n",
       "  '+1:word.position': 9,\n",
       "  '+1:word.letters': 'k u ̀ ʼ',\n",
       "  '+1:word.normalized': 'kùʼ',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 4,\n",
       "  '+1:word.lower()': 'kùʼ',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'kùʼ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̀',\n",
       "  'word.normalized': 'kùʼ',\n",
       "  'word.position': 9,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'kùʼ',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'ku',\n",
       "  'word.root': 'ʼ',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'k u ̀ ʼ',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'Sǐ',\n",
       "  '-1:word.position': 8,\n",
       "  '-1:word.letters': 'S i ̌',\n",
       "  '-1:word.normalized': 'Sǐ',\n",
       "  '-1:word.start_with_capital': 1,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'sǐ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'Si',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': '.',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': '.',\n",
       "  '+1:word.position': 10,\n",
       "  '+1:word.letters': '.',\n",
       "  '+1:word.normalized': '.',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 1,\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 1,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 1},\n",
       " {'word': '.',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': '.',\n",
       "  'word.position': 10,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': '.',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 0,\n",
       "  'word.prefix': '',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 1,\n",
       "  'word.letters': '.',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 1,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'kùʼ',\n",
       "  '-1:word.position': 9,\n",
       "  '-1:word.letters': 'k u ̀ ʼ',\n",
       "  '-1:word.normalized': 'kùʼ',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 4,\n",
       "  '-1:word.lower()': 'kùʼ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 1,\n",
       "  '-1:word.prefix': '',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': '',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': '',\n",
       "  '+1:word.position': 11,\n",
       "  '+1:word.letters': -1,\n",
       "  '+1:word.normalized': '',\n",
       "  '+1:word.start_with_capital': -1,\n",
       "  '+1:len(word+1)': -1,\n",
       "  '+1:word.lower()': '',\n",
       "  '+1:word.isdigit()': -1,\n",
       "  '+1:word.ispunctuation': -1,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 1}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_estimator = SangkakPosFeaturisation()\n",
    "feature_estimator.fit([])\n",
    "\n",
    "Xtrain = feature_estimator.transform(list_train_data)\n",
    "Xdev  = feature_estimator.transform(list_dev_data)\n",
    "Xtest = feature_estimator.transform(list_test_data)\n",
    "\n",
    "ytrain = feature_estimator.transform(list_train_data, label=True)\n",
    "ydev   = feature_estimator.transform(list_dev_data, label=True)\n",
    "ytest  = feature_estimator.transform(list_test_data, label=True)\n",
    "\n",
    "Xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([str(x) for y in Xtrain for x in y]), len([str(x) for y in ytrain for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_train = pd.concat([pd.DataFrame([json.dumps(x) for y in Xtrain for x in y], columns=[\"features\"]), \n",
    "                            pd.DataFrame([x for y in ytrain for x in y], columns=[\"labels\"])], axis=1, ignore_index=True)\n",
    "all_data_dev = pd.concat([pd.DataFrame([json.dumps(x) for y in Xdev for x in y], columns=[\"features\"]), \n",
    "                            pd.DataFrame([x for y in ydev for x in y], columns=[\"labels\"])], axis=1, ignore_index=True)\n",
    "all_data_test = pd.concat([pd.DataFrame([json.dumps(x) for y in Xtest for x in y], columns=[\"features\"]), \n",
    "                            pd.DataFrame([x for y in ytest for x in y], columns=[\"labels\"])], axis=1, ignore_index=True)\n",
    "\n",
    "all_data_parse = pd.concat([all_data_train, all_data_dev, all_data_test], \n",
    "                        axis=0, ignore_index=True)\n",
    "\n",
    "all_data_parse.columns = ['features','labels']\n",
    "all_data_parse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Analyzing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unused / non performants variables\n",
    "remove_unused_features = ['+1:word.isdigit()', '+1:word.ispunctuation', '-1:word.EOS',\n",
    "        '+1:word.BOS', 'word.has_hyphen', '+1:word.EOS', '-1:word.BOS',\n",
    "        '+1:word.EOS', '-1:word.isdigit()', '+1:word.BOS', \n",
    "        '-1:word.ispunctuation', '-1:word.BOS', '+1:word.normalized',\n",
    "        '-1:word.EOS', '-1:word.tag', '+1:word.tag', \n",
    "        '-1:word.start_with_capital','+1:word.start_with_capital']\n",
    "for x in remove_unused_features:\n",
    "    try: del all_data_parse[x]\n",
    "    except: print(\"-- fail to removed: %s\" %x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Preprocess data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    all_data_parse.drop('labels', axis=1).copy(),\n",
    "    all_data_parse['labels'].copy(),\n",
    "    test_size=0.2, random_state=None, shuffle=False\n",
    ")\n",
    "\n",
    "Xtrain, Xdev, ytrain, ydev = train_test_split(\n",
    "    Xtrain, ytrain, test_size=0.25, \n",
    "    random_state=None, shuffle=False\n",
    ")\n",
    "\n",
    "num_class = len(list(set(all_data_parse['labels'])))\n",
    "print(\"Number of classes: %s\" %num_class)\n",
    "\n",
    "print(\"Type of target of ytrain data set: %s\" %type_of_target(ytrain))\n",
    "print(\"Type of target of ytest data set: %s\\n\" %type_of_target(ytest))\n",
    "\n",
    "len_data = len(all_data_parse.index)\n",
    "\n",
    "def f_len(data):\n",
    "    l = len(data)\n",
    "    percent = l*100/len_data\n",
    "    return {'l':l, 'p':int(percent)}\n",
    "\n",
    "print(\"- len of Xtrain data set: {l} ({p}%)\".format(**f_len(Xtrain)))\n",
    "print(\"- len of Xtest data set: {l} ({p}%)\".format(**f_len(Xtest)))\n",
    "print(\"- len of Xdev data set: {l} ({p}%)\".format(**f_len(Xdev)))\n",
    "print(\"len of ytrain data set: {l} ({p}%)\".format(**f_len(ytrain)))\n",
    "print(\"len of ytest data set: {l} ({p}%)\".format(**f_len(ytest)))\n",
    "print(\"len of ydev data set: {l} ({p}%)\".format(**f_len(ydev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[json.loads(x[0]) for x in [x for x in Xtrain.values]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[json.loads(x[0])] for x in [x for x in Xtrain.values]]\n",
    "X_test  = [[json.loads(x[0])] for x in [x for x in Xtest.values]]\n",
    "X_dev   = [[json.loads(x[0])] for x in [x for x in Xdev.values]]\n",
    "y_dev   = [[x] for x in ydev]\n",
    "y_train = [[x] for x in ytrain]\n",
    "y_test  = [[x] for x in ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling with CRF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pycrfsuite\n",
    "import sklearn_crfsuite\n",
    "import math, string, re\n",
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn_crfsuite import metrics\n",
    "from collections import Counter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initialisation of pycrfsuite with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 11131/11131 [00:04<00:00, 2344.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading dev data to CRFsuite: 100%|██████████| 1908/1908 [00:00<00:00, 2249.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout group: 2\n",
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 1\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 596087\n",
      "Seconds required: 14.663\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.092051\n",
      "c2: 0.032877\n",
      "num_memories: 10000\n",
      "max_iterations: 100\n",
      "epsilon: 0.001000\n",
      "stop: 10\n",
      "delta: 0.000100\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 100000\n",
      "\n",
      "Iter 1   time=4.22  loss=543231.08 active=590214 precision=0.013  recall=0.062  F1=0.021  Acc(item/seq)=0.203 0.000  feature_norm=0.12\n",
      "Iter 2   time=0.78  loss=532849.78 active=589501 precision=0.056  recall=0.067  F1=0.028  Acc(item/seq)=0.173 0.000  feature_norm=0.13\n",
      "Iter 3   time=0.73  loss=529426.66 active=592085 precision=0.033  recall=0.069  F1=0.036  Acc(item/seq)=0.209 0.000  feature_norm=0.13\n",
      "Iter 4   time=0.72  loss=524137.82 active=591553 precision=0.098  recall=0.063  F1=0.023  Acc(item/seq)=0.205 0.000  feature_norm=0.17\n",
      "Iter 5   time=0.72  loss=517133.97 active=591798 precision=0.029  recall=0.063  F1=0.023  Acc(item/seq)=0.204 0.000  feature_norm=0.22\n",
      "Iter 6   time=0.82  loss=499300.04 active=590468 precision=0.117  recall=0.111  F1=0.091  Acc(item/seq)=0.262 0.000  feature_norm=0.49\n",
      "Iter 7   time=0.77  loss=487438.78 active=591627 precision=0.104  recall=0.133  F1=0.099  Acc(item/seq)=0.257 0.000  feature_norm=0.80\n",
      "Iter 8   time=0.91  loss=473479.16 active=591391 precision=0.125  recall=0.128  F1=0.103  Acc(item/seq)=0.279 0.000  feature_norm=1.00\n",
      "Iter 9   time=0.73  loss=459110.83 active=587656 precision=0.156  recall=0.164  F1=0.141  Acc(item/seq)=0.318 0.000  feature_norm=1.38\n",
      "Iter 10  time=0.75  loss=438763.22 active=587340 precision=0.152  recall=0.158  F1=0.133  Acc(item/seq)=0.334 0.000  feature_norm=1.90\n",
      "Iter 11  time=0.73  loss=410977.79 active=581711 precision=0.254  recall=0.185  F1=0.163  Acc(item/seq)=0.357 0.000  feature_norm=2.89\n",
      "Iter 12  time=0.74  loss=376149.79 active=577903 precision=0.412  recall=0.208  F1=0.202  Acc(item/seq)=0.400 0.000  feature_norm=4.40\n",
      "Iter 13  time=0.73  loss=330580.22 active=548693 precision=0.425  recall=0.290  F1=0.293  Acc(item/seq)=0.490 0.000  feature_norm=6.75\n",
      "Iter 14  time=0.85  loss=290590.60 active=525656 precision=0.404  recall=0.328  F1=0.338  Acc(item/seq)=0.526 0.000  feature_norm=9.28\n",
      "Iter 15  time=0.82  loss=267930.61 active=517541 precision=0.401  recall=0.360  F1=0.355  Acc(item/seq)=0.566 0.004  feature_norm=11.14\n",
      "Iter 16  time=0.80  loss=249606.37 active=526811 precision=0.482  recall=0.396  F1=0.397  Acc(item/seq)=0.595 0.005  feature_norm=11.72\n",
      "Iter 17  time=0.76  loss=231071.08 active=525455 precision=0.482  recall=0.390  F1=0.392  Acc(item/seq)=0.598 0.005  feature_norm=13.34\n",
      "Iter 18  time=0.76  loss=212471.69 active=517614 precision=0.629  recall=0.447  F1=0.450  Acc(item/seq)=0.648 0.010  feature_norm=14.82\n",
      "Iter 19  time=0.78  loss=194364.88 active=503714 precision=0.621  recall=0.480  F1=0.499  Acc(item/seq)=0.669 0.015  feature_norm=16.84\n",
      "Iter 20  time=0.75  loss=169966.80 active=492707 precision=0.612  recall=0.537  F1=0.554  Acc(item/seq)=0.689 0.018  feature_norm=22.34\n",
      "Iter 21  time=0.92  loss=149211.34 active=495645 precision=0.636  recall=0.584  F1=0.601  Acc(item/seq)=0.731 0.025  feature_norm=24.98\n",
      "Iter 22  time=0.77  loss=137917.79 active=495915 precision=0.643  recall=0.596  F1=0.615  Acc(item/seq)=0.743 0.026  feature_norm=27.53\n",
      "Iter 23  time=0.76  loss=112909.76 active=474909 precision=0.663  recall=0.636  F1=0.647  Acc(item/seq)=0.773 0.040  feature_norm=34.56\n",
      "Iter 24  time=0.72  loss=95563.99 active=475799 precision=0.687  recall=0.652  F1=0.661  Acc(item/seq)=0.793 0.055  feature_norm=43.57\n",
      "Iter 25  time=0.71  loss=78847.40 active=476285 precision=0.704  recall=0.666  F1=0.682  Acc(item/seq)=0.816 0.064  feature_norm=49.98\n",
      "Iter 26  time=0.77  loss=68055.62 active=462720 precision=0.721  recall=0.689  F1=0.702  Acc(item/seq)=0.832 0.080  feature_norm=56.68\n",
      "Iter 27  time=1.12  loss=55445.81 active=454310 precision=0.730  recall=0.693  F1=0.708  Acc(item/seq)=0.836 0.083  feature_norm=68.03\n",
      "Iter 28  time=0.92  loss=45887.00 active=437142 precision=0.717  recall=0.712  F1=0.711  Acc(item/seq)=0.839 0.111  feature_norm=80.85\n",
      "Iter 29  time=0.73  loss=38779.53 active=430743 precision=0.725  recall=0.713  F1=0.717  Acc(item/seq)=0.846 0.117  feature_norm=88.55\n",
      "Iter 30  time=0.73  loss=32058.88 active=408166 precision=0.732  recall=0.718  F1=0.724  Acc(item/seq)=0.852 0.121  feature_norm=99.93\n",
      "Iter 31  time=0.75  loss=25931.35 active=404914 precision=0.738  recall=0.722  F1=0.729  Acc(item/seq)=0.856 0.127  feature_norm=112.38\n",
      "Iter 32  time=0.73  loss=22550.82 active=385569 precision=0.728  recall=0.710  F1=0.716  Acc(item/seq)=0.851 0.105  feature_norm=126.05\n",
      "Iter 33  time=0.84  loss=18762.81 active=384465 precision=0.736  recall=0.714  F1=0.724  Acc(item/seq)=0.855 0.106  feature_norm=131.70\n",
      "Iter 34  time=0.84  loss=16652.92 active=333194 precision=0.737  recall=0.715  F1=0.724  Acc(item/seq)=0.855 0.108  feature_norm=137.76\n",
      "Iter 35  time=0.77  loss=14470.48 active=322852 precision=0.741  recall=0.725  F1=0.732  Acc(item/seq)=0.859 0.103  feature_norm=145.79\n",
      "Iter 36  time=0.73  loss=12973.59 active=318349 precision=0.743  recall=0.726  F1=0.733  Acc(item/seq)=0.862 0.110  feature_norm=150.31\n",
      "Iter 37  time=0.79  loss=11877.41 active=316008 precision=0.738  recall=0.727  F1=0.732  Acc(item/seq)=0.861 0.100  feature_norm=152.63\n",
      "Iter 38  time=0.82  loss=10962.85 active=289608 precision=0.745  recall=0.732  F1=0.738  Acc(item/seq)=0.863 0.107  feature_norm=154.11\n",
      "Iter 39  time=1.00  loss=10333.30 active=287166 precision=0.739  recall=0.732  F1=0.735  Acc(item/seq)=0.859 0.109  feature_norm=153.46\n",
      "Iter 40  time=1.13  loss=9729.37  active=268496 precision=0.743  recall=0.731  F1=0.736  Acc(item/seq)=0.859 0.109  feature_norm=153.49\n",
      "Iter 41  time=0.79  loss=9192.84  active=262672 precision=0.745  recall=0.731  F1=0.737  Acc(item/seq)=0.860 0.110  feature_norm=153.19\n",
      "Iter 42  time=0.78  loss=8684.44  active=209742 precision=0.746  recall=0.729  F1=0.736  Acc(item/seq)=0.858 0.110  feature_norm=154.21\n",
      "Iter 43  time=0.81  loss=8308.07  active=176046 precision=0.741  recall=0.724  F1=0.731  Acc(item/seq)=0.858 0.107  feature_norm=155.40\n",
      "Iter 44  time=0.79  loss=7956.74  active=145085 precision=0.740  recall=0.728  F1=0.733  Acc(item/seq)=0.858 0.115  feature_norm=157.29\n",
      "Iter 45  time=0.91  loss=7624.21  active=134619 precision=0.739  recall=0.726  F1=0.732  Acc(item/seq)=0.857 0.115  feature_norm=159.86\n",
      "Iter 46  time=1.00  loss=7367.06  active=116863 precision=0.737  recall=0.725  F1=0.730  Acc(item/seq)=0.856 0.114  feature_norm=162.07\n",
      "Iter 47  time=0.86  loss=7164.66  active=108770 precision=0.737  recall=0.724  F1=0.730  Acc(item/seq)=0.856 0.115  feature_norm=163.80\n",
      "Iter 48  time=0.82  loss=6965.57  active=104315 precision=0.737  recall=0.723  F1=0.729  Acc(item/seq)=0.854 0.117  feature_norm=165.44\n",
      "Iter 49  time=0.82  loss=6815.88  active=102414 precision=0.738  recall=0.724  F1=0.730  Acc(item/seq)=0.855 0.112  feature_norm=166.52\n",
      "Iter 50  time=0.85  loss=6659.64  active=94996 precision=0.737  recall=0.724  F1=0.730  Acc(item/seq)=0.853 0.107  feature_norm=167.80\n",
      "Iter 51  time=1.01  loss=6557.12  active=93864 precision=0.737  recall=0.723  F1=0.729  Acc(item/seq)=0.852 0.116  feature_norm=168.50\n",
      "Iter 52  time=0.90  loss=6453.14  active=92020 precision=0.737  recall=0.723  F1=0.729  Acc(item/seq)=0.853 0.110  feature_norm=169.30\n",
      "Iter 53  time=0.85  loss=6363.31  active=90944 precision=0.739  recall=0.724  F1=0.730  Acc(item/seq)=0.853 0.105  feature_norm=169.77\n",
      "Iter 54  time=0.88  loss=6266.15  active=89666 precision=0.738  recall=0.722  F1=0.729  Acc(item/seq)=0.853 0.104  feature_norm=170.22\n",
      "Iter 55  time=0.94  loss=6180.47  active=87975 precision=0.737  recall=0.722  F1=0.728  Acc(item/seq)=0.851 0.098  feature_norm=170.35\n",
      "Iter 56  time=0.90  loss=6095.82  active=86004 precision=0.741  recall=0.725  F1=0.732  Acc(item/seq)=0.852 0.099  feature_norm=170.57\n",
      "Iter 57  time=1.08  loss=6028.81  active=83441 precision=0.740  recall=0.725  F1=0.732  Acc(item/seq)=0.853 0.103  feature_norm=170.70\n",
      "Iter 58  time=0.86  loss=5964.86  active=82254 precision=0.737  recall=0.722  F1=0.728  Acc(item/seq)=0.850 0.098  feature_norm=170.84\n",
      "Iter 59  time=0.83  loss=5902.44  active=81103 precision=0.739  recall=0.722  F1=0.729  Acc(item/seq)=0.850 0.107  feature_norm=170.97\n",
      "Iter 60  time=0.87  loss=5845.25  active=79969 precision=0.737  recall=0.720  F1=0.727  Acc(item/seq)=0.848 0.104  feature_norm=171.05\n",
      "Iter 61  time=0.88  loss=5792.08  active=78119 precision=0.739  recall=0.722  F1=0.729  Acc(item/seq)=0.850 0.104  feature_norm=171.09\n",
      "Iter 62  time=0.89  loss=5740.00  active=76548 precision=0.737  recall=0.720  F1=0.727  Acc(item/seq)=0.848 0.104  feature_norm=171.14\n",
      "Iter 63  time=1.21  loss=5694.66  active=75302 precision=0.738  recall=0.720  F1=0.727  Acc(item/seq)=0.849 0.104  feature_norm=171.30\n",
      "Iter 64  time=1.02  loss=5658.44  active=74696 precision=0.738  recall=0.719  F1=0.727  Acc(item/seq)=0.848 0.103  feature_norm=171.55\n",
      "Iter 65  time=0.87  loss=5626.41  active=73695 precision=0.739  recall=0.719  F1=0.727  Acc(item/seq)=0.849 0.105  feature_norm=171.83\n",
      "Iter 66  time=0.90  loss=5596.25  active=72842 precision=0.738  recall=0.718  F1=0.726  Acc(item/seq)=0.847 0.107  feature_norm=172.17\n",
      "Iter 67  time=0.87  loss=5568.45  active=72186 precision=0.740  recall=0.718  F1=0.727  Acc(item/seq)=0.848 0.107  feature_norm=172.52\n",
      "Iter 68  time=1.07  loss=5544.58  active=71488 precision=0.739  recall=0.718  F1=0.727  Acc(item/seq)=0.848 0.108  feature_norm=172.90\n",
      "Iter 69  time=0.87  loss=5523.01  active=70978 precision=0.739  recall=0.718  F1=0.727  Acc(item/seq)=0.848 0.106  feature_norm=173.13\n",
      "Iter 70  time=0.89  loss=5501.23  active=70273 precision=0.741  recall=0.718  F1=0.728  Acc(item/seq)=0.848 0.106  feature_norm=173.27\n",
      "Iter 71  time=0.93  loss=5478.50  active=69327 precision=0.740  recall=0.719  F1=0.727  Acc(item/seq)=0.849 0.113  feature_norm=173.35\n",
      "Iter 72  time=0.87  loss=5459.85  active=68488 precision=0.742  recall=0.719  F1=0.729  Acc(item/seq)=0.849 0.115  feature_norm=173.35\n",
      "Iter 73  time=1.11  loss=5442.68  active=67975 precision=0.739  recall=0.719  F1=0.727  Acc(item/seq)=0.848 0.115  feature_norm=173.30\n",
      "Iter 74  time=1.03  loss=5425.56  active=67736 precision=0.741  recall=0.719  F1=0.728  Acc(item/seq)=0.847 0.113  feature_norm=173.11\n",
      "Iter 75  time=0.94  loss=5408.23  active=67200 precision=0.736  recall=0.717  F1=0.725  Acc(item/seq)=0.846 0.115  feature_norm=172.87\n",
      "Iter 76  time=0.86  loss=5393.28  active=66789 precision=0.741  recall=0.720  F1=0.728  Acc(item/seq)=0.848 0.118  feature_norm=172.60\n",
      "Iter 77  time=0.86  loss=5377.99  active=66301 precision=0.738  recall=0.717  F1=0.726  Acc(item/seq)=0.847 0.117  feature_norm=172.45\n",
      "Iter 78  time=0.92  loss=5364.50  active=66028 precision=0.739  recall=0.719  F1=0.727  Acc(item/seq)=0.848 0.120  feature_norm=172.22\n",
      "Iter 79  time=1.11  loss=5349.72  active=65605 precision=0.738  recall=0.718  F1=0.726  Acc(item/seq)=0.847 0.117  feature_norm=171.97\n",
      "Iter 80  time=0.94  loss=5337.53  active=65058 precision=0.739  recall=0.718  F1=0.726  Acc(item/seq)=0.847 0.120  feature_norm=171.74\n",
      "Iter 81  time=0.96  loss=5324.96  active=64472 precision=0.739  recall=0.719  F1=0.726  Acc(item/seq)=0.848 0.117  feature_norm=171.66\n",
      "Iter 82  time=0.91  loss=5313.90  active=64155 precision=0.738  recall=0.718  F1=0.726  Acc(item/seq)=0.847 0.115  feature_norm=171.55\n",
      "Iter 83  time=0.94  loss=5302.43  active=63949 precision=0.736  recall=0.719  F1=0.725  Acc(item/seq)=0.847 0.117  feature_norm=171.48\n",
      "Iter 84  time=1.19  loss=5291.85  active=63810 precision=0.735  recall=0.718  F1=0.724  Acc(item/seq)=0.847 0.111  feature_norm=171.43\n",
      "Iter 85  time=0.93  loss=5282.60  active=63629 precision=0.736  recall=0.720  F1=0.726  Acc(item/seq)=0.848 0.114  feature_norm=171.44\n",
      "Iter 86  time=0.90  loss=5273.20  active=63366 precision=0.735  recall=0.719  F1=0.725  Acc(item/seq)=0.847 0.111  feature_norm=171.48\n",
      "Iter 87  time=0.91  loss=5264.38  active=63015 precision=0.736  recall=0.720  F1=0.725  Acc(item/seq)=0.848 0.111  feature_norm=171.51\n",
      "Iter 88  time=0.91  loss=5254.99  active=62679 precision=0.736  recall=0.719  F1=0.726  Acc(item/seq)=0.847 0.115  feature_norm=171.51\n",
      "Iter 89  time=1.15  loss=5247.32  active=62272 precision=0.737  recall=0.720  F1=0.727  Acc(item/seq)=0.849 0.114  feature_norm=171.56\n",
      "Iter 90  time=0.97  loss=5238.44  active=62032 precision=0.736  recall=0.720  F1=0.726  Acc(item/seq)=0.848 0.115  feature_norm=171.59\n",
      "Iter 91  time=0.90  loss=5231.53  active=61925 precision=0.738  recall=0.720  F1=0.727  Acc(item/seq)=0.849 0.118  feature_norm=171.59\n",
      "Iter 92  time=0.93  loss=5223.98  active=61811 precision=0.737  recall=0.720  F1=0.726  Acc(item/seq)=0.848 0.118  feature_norm=171.55\n",
      "Iter 93  time=0.88  loss=5217.60  active=61344 precision=0.738  recall=0.720  F1=0.727  Acc(item/seq)=0.849 0.118  feature_norm=171.47\n",
      "Iter 94  time=1.12  loss=5210.67  active=60929 precision=0.736  recall=0.720  F1=0.726  Acc(item/seq)=0.847 0.119  feature_norm=171.40\n",
      "Iter 95  time=1.18  loss=5204.28  active=60722 precision=0.737  recall=0.721  F1=0.727  Acc(item/seq)=0.847 0.119  feature_norm=171.36\n",
      "Iter 96  time=0.91  loss=5198.52  active=60516 precision=0.736  recall=0.722  F1=0.727  Acc(item/seq)=0.847 0.116  feature_norm=171.25\n",
      "Iter 97  time=0.97  loss=5192.56  active=60197 precision=0.736  recall=0.720  F1=0.726  Acc(item/seq)=0.847 0.119  feature_norm=171.13\n",
      "Iter 98  time=1.02  loss=5187.48  active=59902 precision=0.735  recall=0.720  F1=0.725  Acc(item/seq)=0.847 0.116  feature_norm=170.98\n",
      "Iter 99  time=1.09  loss=5182.06  active=59805 precision=0.737  recall=0.721  F1=0.727  Acc(item/seq)=0.847 0.117  feature_norm=170.88\n",
      "Iter 100 time=1.02  loss=5176.71  active=59814 precision=0.735  recall=0.719  F1=0.725  Acc(item/seq)=0.847 0.116  feature_norm=170.81\n",
      "================================================\n",
      "Label      Precision    Recall     F1    Support\n",
      "-------  -----------  --------  -----  ---------\n",
      "ADJ            0.783     0.732  0.757        859\n",
      "ADP            0.871     0.850  0.860       1877\n",
      "ADV            0.823     0.613  0.703        949\n",
      "AUX            0.827     0.780  0.803       1933\n",
      "CCONJ          0.677     0.797  0.732        533\n",
      "DET            0.830     0.844  0.837       3132\n",
      "INTJ           0.000     0.000  0.000         32\n",
      "NOUN           0.836     0.842  0.839       7270\n",
      "NUM            0.919     0.761  0.832        535\n",
      "PART           0.730     0.778  0.753       2184\n",
      "PRON           0.872     0.868  0.870       3805\n",
      "PROPN          0.846     0.888  0.866       3025\n",
      "PUNCT          1.000     0.993  0.997       2428\n",
      "SCONJ          0.910     0.899  0.904       1638\n",
      "SYM                             0.000          0\n",
      "VERB           0.844     0.865  0.854       5601\n",
      "------------------------------------------------\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 91.985\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 59814 (596087)\n",
      "Number of active attributes: 23823 (42338)\n",
      "Number of active labels: 16 (16)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.022\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/multi/crf_sangkak-bbj_2023-09-22_21:45:56.610246.object']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = f\"sangkak-{language}\"\n",
    "build_date = str(datetime.now()).replace(' ','_')\n",
    "model_name = Path(f\"models/multi/crf_{project}_{build_date}.model\")\n",
    "model_file = str(model_name)\n",
    "file_crf = Path(f\"models/multi/crf_{project}_{build_date}.object\")\n",
    "\n",
    "params = {\n",
    "    \"algorithm\": 'lbfgs',\n",
    "    \"c1\": 0.0920512484757745,\n",
    "    \"c2\": 0.0328771171605105, \n",
    "    \"max_iterations\":100,\n",
    "    \"verbose\": True,\n",
    "    \"num_memories\":10000,\n",
    "    \"epsilon\": 1e-3,\n",
    "    \"linesearch\": \"MoreThuente\",\n",
    "    \"max_linesearch\":100000,\n",
    "    \"delta\":1e-4,\n",
    "    #n_job=-1,\n",
    "    #\"c\": 2,\n",
    "    #\"pa_type\": 2,\n",
    "    \"all_possible_states\":True,\n",
    "    \"all_possible_transitions\":True, \n",
    "    \"model_filename\": model_file\n",
    "}\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(**params)\n",
    "\n",
    "crf.fit(Xtrain, ytrain, Xdev, ydev)    \n",
    "\n",
    "final = {\"crf\": crf, \"params\": params}\n",
    "joblib.dump(final, file_crf) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_crf_model(crf, Xtest, ytest):\n",
    "    # get model classes\n",
    "    labels = list(crf.classes_)\n",
    "    #labels.remove('O')\n",
    "\n",
    "    sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
    "    #print(sorted_labels)\n",
    "\n",
    "    # obtaining metrics such as accuracy, etc. on the test set\n",
    "    ypred = crf.predict(Xtest)\n",
    "    print('- F1 score on the test set = {}'.format(\n",
    "            metrics.flat_f1_score(ytest, ypred, average='weighted', \n",
    "                        labels=labels, zero_division=False)))\n",
    "\n",
    "    print('- Accuracy on the test set = {}\\n'.format(\n",
    "        metrics.flat_accuracy_score(ytest, ypred)))\n",
    "\n",
    "    print('Train set classification report: \\n\\n{}'.format(\n",
    "                metrics.flat_classification_report(ytest, \n",
    "                ypred, labels=sorted_labels, digits=3, zero_division=False)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_crf_model(crf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(pycrfsuite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with no augmentation\n",
    "y_pred = [tagger.tag(xseq) for xseq in Xtest]\n",
    "\n",
    "print(bio_classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to find the best dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "params_skf = {\n",
    "    \"algorithm\": 'lbfgs',\n",
    "    \"c1\": 0.0920512484757745,\n",
    "    \"c2\": 0.0328771171605105, \n",
    "    \"max_iterations\":100,\n",
    "    \"verbose\":False,\n",
    "    \"num_memories\":1000,\n",
    "    \"epsilon\": 1e-4,\n",
    "    \"linesearch\": \"MoreThuente\",\n",
    "    \"max_linesearch\":100000,\n",
    "    \"delta\":1e-3,\n",
    "    \"all_possible_states\":True,\n",
    "    \"all_possible_transitions\":True\n",
    "}\n",
    "\n",
    "crf_skf = sklearn_crfsuite.CRF(**params_skf)\n",
    "\n",
    "train_split = all_data_parse.drop('labels', axis=1).copy()\n",
    "test_split = all_data_parse['labels'].copy()\n",
    "\n",
    "for train_index, test_index in skf.split(train_split, test_split):\n",
    "    print('Working on Fold ', str(fold_no),': ')\n",
    "    train = all_data_parse.loc[train_index,:]\n",
    "    test  = all_data_parse.loc[test_index,:]\n",
    "\n",
    "    Xx_train = [[x] for x in [x for x in train[\"features\"].values]]\n",
    "    yy_train = [[x] for x in train['labels'].values]\n",
    "\n",
    "    Xx_test = [[x] for x in [x for x in test[\"features\"].values]]\n",
    "    yy_test = [[x] for x in test['labels'].values]\n",
    "\n",
    "    print(f'\\tTraining: {len(Xx_train)} / {len(yy_train)}')\n",
    "    crf_skf.fit(Xx_train, yy_train, X_dev, y_dev)\n",
    "\n",
    "    print(f'\\tTest: {len(Xx_test)} / {len(yy_test)}')\n",
    "    predictions = crf_skf.predict(Xx_test)\n",
    "\n",
    "    print('\\t', '>> f1_score: ', \n",
    "                f1_score(yy_test, predictions, average=\"macro\"), '\\n')\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Grid search\n",
    "\n",
    "In other to optimised parameters of CRF model, we want here to find the best parameters that will fit to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import scorers\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial model parameters and delete c1 and c2 parameters\n",
    "del params['c1']\n",
    "del params['c2']\n",
    "\n",
    "crf_grill = sklearn_crfsuite.CRF(**params)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.1),\n",
    "    'c2': scipy.stats.expon(scale=0.05)\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf_grill, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=5,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rs.cv_results_)\n",
    "_x = [s['c1'] for s in rs.cv_results_['params']]\n",
    "_y = [s['c2'] for s in rs.cv_results_['params']]\n",
    "_c = [s for s in rs.cv_results_['mean_score_time']]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 12)\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('C1')\n",
    "ax.set_ylabel('C2')\n",
    "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
    "    min(_c), max(_c)\n",
    "))\n",
    "\n",
    "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_grid = rs.best_estimator_\n",
    "y_pred = crf_grid.predict(Xtest)\n",
    "print(metrics.flat_classification_report(\n",
    "    ytest, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia-crf-lf-OXAjte5Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "646b439550bec9cacc5e0384422c9ee78f8df74b182cfe1fc7410e07b34d6961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
