{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sangkak AI Challenge: POS tasks\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author**: Elvis MBONING (NTeALan Research and Development Team)\n",
    "- **Session**: septembre 2023\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we try to train differents models.\n",
    "\n",
    "We want to train these hypothesis for CRF_suite and Xgboost model:\n",
    "\n",
    "- impact of features normalization to the model classification\n",
    "- impact of features regulatization to the model classification\n",
    "- impact of choice of classification algorithm to the model classification\n",
    "- impact of data augmentation based on position (imbalence classes) to the model classification\n",
    "- impact of data augmentation based on features (imbalence classes) to the model classification\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Impact of features normalization to the model classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sangkak_estimators import SangkakPosProjetReader, SangkakPosFeaturisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Case of Xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (1.7.6)\n",
      "Requirement already satisfied: dython in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (0.7.4)\n",
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from xgboost) (1.10.0)\n",
      "Requirement already satisfied: numpy in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from xgboost) (1.24.2)\n",
      "Requirement already satisfied: pandas>=1.4.2 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from dython) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.5.3 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from dython) (3.6.3)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from dython) (1.2.1)\n",
      "Requirement already satisfied: scikit-plot>=0.3.7 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from dython) (0.3.7)\n",
      "Requirement already satisfied: psutil>=5.9.1 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from dython) (5.9.4)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from dython) (0.12.2)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from lazypredict) (4.64.1)\n",
      "Requirement already satisfied: joblib in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from lazypredict) (1.2.0)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.0.0-py3-none-manylinux_2_28_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from matplotlib>=3.5.3->dython) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from pandas>=1.4.2->dython) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from scikit-learn>=0.24.2->dython) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/elvis/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.5.3->dython) (1.16.0)\n",
      "Installing collected packages: click, lightgbm, lazypredict\n",
      "Successfully installed click-8.1.7 lazypredict-0.2.12 lightgbm-4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Install python packages dependencies (if not already installed)\n",
    "# version=xgboost-1.7.6\n",
    "!pip3 install dython lazypredict -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12347 12347 12347\n",
      "2447 2447 2447\n",
      "9315 9315 9315\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mwɔ̌ʼ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pfʉ́tə́</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nə́</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mwâsi</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>máp</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12342</th>\n",
       "      <td>751</td>\n",
       "      <td>kə</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12343</th>\n",
       "      <td>751</td>\n",
       "      <td>fǎʼ</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12344</th>\n",
       "      <td>751</td>\n",
       "      <td>nə́</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12345</th>\n",
       "      <td>751</td>\n",
       "      <td>é</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12346</th>\n",
       "      <td>751</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12347 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id     word   tags\n",
       "0                1    Mwɔ̌ʼ   NOUN\n",
       "1                1  pfʉ́tə́   VERB\n",
       "2                1      nə́    ADP\n",
       "3                1   mwâsi   NOUN\n",
       "4                1     máp    DET\n",
       "...            ...      ...    ...\n",
       "12342          751       kə    AUX\n",
       "12343          751     fǎʼ   VERB\n",
       "12344          751      nə́    ADP\n",
       "12345          751       é   PRON\n",
       "12346          751        .  PUNCT\n",
       "\n",
       "[12347 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get path of test data \n",
    "language = 'bbj'\n",
    "bbj_pos_path   = Path(f'../data_source/masakhane-pos/data/{language}')\n",
    "train_data_path = bbj_pos_path / 'train.txt'\n",
    "dev_data_path = bbj_pos_path / 'dev.txt'\n",
    "test_data_path = bbj_pos_path / 'test.txt'\n",
    "\n",
    "# read data from source with sklearn estimator\n",
    "reader_estimator = SangkakPosProjetReader()\n",
    "list_train_data, pd_train_data = reader_estimator.fit(train_data_path).transform_analysis()\n",
    "list_dev_data, pd_dev_data = reader_estimator.fit(dev_data_path).transform_analysis()\n",
    "list_test_data, pd_test_data = reader_estimator.fit(test_data_path).transform_analysis()\n",
    "\n",
    "pd_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Mwɔ̌ʼ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̌',\n",
       "  'word.normalized': 'Mwɔ̌ʼ',\n",
       "  'word.position': 0,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'mwɔ̌ʼ',\n",
       "  'word.start_with_capital': -1,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'Mw',\n",
       "  'word.root': '̌ʼ',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': -1,\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 1,\n",
       "  '-1:word': '',\n",
       "  '-1:word.position': -1,\n",
       "  '-1:word.tag': '',\n",
       "  '-1:word.letters': -1,\n",
       "  '-1:word.normalized': '',\n",
       "  '-1:word.start_with_capital': -1,\n",
       "  '-1:len(word-1)': -1,\n",
       "  '-1:word.lower()': '',\n",
       "  '-1:word.isdigit()': -1,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': '',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'pf',\n",
       "  '+1:word.root': '́tə́',\n",
       "  '+1:word': 'pfʉ́tə́',\n",
       "  '+1:word.tag': 'VERB',\n",
       "  '+1:word.position': 1,\n",
       "  '+1:word.letters': 'p f ʉ ́ t ə ́',\n",
       "  '+1:word.normalized': 'pfʉ́tə́',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 7,\n",
       "  '+1:word.lower()': 'pfʉ́tə́',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'pfʉ́tə́',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'pfʉ́tə́',\n",
       "  'word.position': 1,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'pfʉ́tə́',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'pf',\n",
       "  'word.root': '́tə́',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'p f ʉ ́ t ə ́',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'Mwɔ̌ʼ',\n",
       "  '-1:word.position': 0,\n",
       "  '-1:word.tag': 'NOUN',\n",
       "  '-1:word.letters': 'M w ɔ ̌ ʼ',\n",
       "  '-1:word.normalized': 'Mwɔ̌ʼ',\n",
       "  '-1:word.start_with_capital': 1,\n",
       "  '-1:len(word-1)': 5,\n",
       "  '-1:word.lower()': 'mwɔ̌ʼ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 1,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'Mw',\n",
       "  '-1:word.root': '̌ʼ',\n",
       "  '+1:word.prefix': 'nə',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'nə́',\n",
       "  '+1:word.tag': 'ADP',\n",
       "  '+1:word.position': 2,\n",
       "  '+1:word.letters': 'n ə ́',\n",
       "  '+1:word.normalized': 'nə́',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'nə́',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'nə́',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'nə́',\n",
       "  'word.position': 2,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'nə́',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'nə',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'n ə ́',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'pfʉ́tə́',\n",
       "  '-1:word.position': 1,\n",
       "  '-1:word.tag': 'VERB',\n",
       "  '-1:word.letters': 'p f ʉ ́ t ə ́',\n",
       "  '-1:word.normalized': 'pfʉ́tə́',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 7,\n",
       "  '-1:word.lower()': 'pfʉ́tə́',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'pf',\n",
       "  '-1:word.root': '́tə́',\n",
       "  '+1:word.prefix': 'mw',\n",
       "  '+1:word.root': '̂si',\n",
       "  '+1:word': 'mwâsi',\n",
       "  '+1:word.tag': 'NOUN',\n",
       "  '+1:word.position': 3,\n",
       "  '+1:word.letters': 'm w a ̂ s i',\n",
       "  '+1:word.normalized': 'mwâsi',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 6,\n",
       "  '+1:word.lower()': 'mwâsi',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'mwâsi',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̂',\n",
       "  'word.normalized': 'mwâsi',\n",
       "  'word.position': 3,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'mwâsi',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'mw',\n",
       "  'word.root': '̂si',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'm w a ̂ s i',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'nə́',\n",
       "  '-1:word.position': 2,\n",
       "  '-1:word.tag': 'ADP',\n",
       "  '-1:word.letters': 'n ə ́',\n",
       "  '-1:word.normalized': 'nə́',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'nə́',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'nə',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'ma',\n",
       "  '+1:word.root': 'p',\n",
       "  '+1:word': 'máp',\n",
       "  '+1:word.tag': 'DET',\n",
       "  '+1:word.position': 4,\n",
       "  '+1:word.letters': 'm a ́ p',\n",
       "  '+1:word.normalized': 'máp',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 4,\n",
       "  '+1:word.lower()': 'máp',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'máp',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'máp',\n",
       "  'word.position': 4,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'máp',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'ma',\n",
       "  'word.root': 'p',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'm a ́ p',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'mwâsi',\n",
       "  '-1:word.position': 3,\n",
       "  '-1:word.tag': 'NOUN',\n",
       "  '-1:word.letters': 'm w a ̂ s i',\n",
       "  '-1:word.normalized': 'mwâsi',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 6,\n",
       "  '-1:word.lower()': 'mwâsi',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'mw',\n",
       "  '-1:word.root': '̂si',\n",
       "  '+1:word.prefix': 'yə',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'yə́',\n",
       "  '+1:word.tag': 'DET',\n",
       "  '+1:word.position': 5,\n",
       "  '+1:word.letters': 'y ə ́',\n",
       "  '+1:word.normalized': 'yə́',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'yə́',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'yə́',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'yə́',\n",
       "  'word.position': 5,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'yə́',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'yə',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'y ə ́',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'máp',\n",
       "  '-1:word.position': 4,\n",
       "  '-1:word.tag': 'DET',\n",
       "  '-1:word.letters': 'm a ́ p',\n",
       "  '-1:word.normalized': 'máp',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 4,\n",
       "  '-1:word.lower()': 'máp',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'ma',\n",
       "  '-1:word.root': 'p',\n",
       "  '+1:word.prefix': 'cw',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'cwə',\n",
       "  '+1:word.tag': 'NOUN',\n",
       "  '+1:word.position': 6,\n",
       "  '+1:word.letters': 'c w ə',\n",
       "  '+1:word.normalized': 'cwə',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'cwə',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'cwə',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'cwə',\n",
       "  'word.position': 6,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'cwə',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 0,\n",
       "  'word.prefix': 'cw',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'c w ə',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'yə́',\n",
       "  '-1:word.position': 5,\n",
       "  '-1:word.tag': 'DET',\n",
       "  '-1:word.letters': 'y ə ́',\n",
       "  '-1:word.normalized': 'yə́',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'yə́',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'yə',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'Cy',\n",
       "  '+1:word.root': 'pɔ',\n",
       "  '+1:word': 'Cyəpɔ',\n",
       "  '+1:word.tag': 'PROPN',\n",
       "  '+1:word.position': 7,\n",
       "  '+1:word.letters': 'C y ə p ɔ',\n",
       "  '+1:word.normalized': 'Cyəpɔ',\n",
       "  '+1:word.start_with_capital': 1,\n",
       "  '+1:len(word+1)': 5,\n",
       "  '+1:word.lower()': 'cyəpɔ',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'Cyəpɔ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': 'Cyəpɔ',\n",
       "  'word.position': 7,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'cyəpɔ',\n",
       "  'word.start_with_capital': 1,\n",
       "  'word.have_tone': 0,\n",
       "  'word.prefix': 'Cy',\n",
       "  'word.root': 'pɔ',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'C y ə p ɔ',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'cwə',\n",
       "  '-1:word.position': 6,\n",
       "  '-1:word.tag': 'NOUN',\n",
       "  '-1:word.letters': 'c w ə',\n",
       "  '-1:word.normalized': 'cwə',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'cwə',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'cw',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': 'Si',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': 'Sǐ',\n",
       "  '+1:word.tag': 'NOUN',\n",
       "  '+1:word.position': 8,\n",
       "  '+1:word.letters': 'S i ̌',\n",
       "  '+1:word.normalized': 'Sǐ',\n",
       "  '+1:word.start_with_capital': 1,\n",
       "  '+1:len(word+1)': 3,\n",
       "  '+1:word.lower()': 'sǐ',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'Sǐ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̌',\n",
       "  'word.normalized': 'Sǐ',\n",
       "  'word.position': 8,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'sǐ',\n",
       "  'word.start_with_capital': 1,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'Si',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'S i ̌',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'Cyəpɔ',\n",
       "  '-1:word.position': 7,\n",
       "  '-1:word.tag': 'PROPN',\n",
       "  '-1:word.letters': 'C y ə p ɔ',\n",
       "  '-1:word.normalized': 'Cyəpɔ',\n",
       "  '-1:word.start_with_capital': 1,\n",
       "  '-1:len(word-1)': 5,\n",
       "  '-1:word.lower()': 'cyəpɔ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'Cy',\n",
       "  '-1:word.root': 'pɔ',\n",
       "  '+1:word.prefix': 'ku',\n",
       "  '+1:word.root': 'ʼ',\n",
       "  '+1:word': 'kùʼ',\n",
       "  '+1:word.tag': 'VERB',\n",
       "  '+1:word.position': 9,\n",
       "  '+1:word.letters': 'k u ̀ ʼ',\n",
       "  '+1:word.normalized': 'kùʼ',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 4,\n",
       "  '+1:word.lower()': 'kùʼ',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 0,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 0},\n",
       " {'word': 'kùʼ',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '̀',\n",
       "  'word.normalized': 'kùʼ',\n",
       "  'word.position': 9,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': 'kùʼ',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 1,\n",
       "  'word.prefix': 'ku',\n",
       "  'word.root': 'ʼ',\n",
       "  'word.ispunctuation': 0,\n",
       "  'word.letters': 'k u ̀ ʼ',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 0,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'Sǐ',\n",
       "  '-1:word.position': 8,\n",
       "  '-1:word.tag': 'NOUN',\n",
       "  '-1:word.letters': 'S i ̌',\n",
       "  '-1:word.normalized': 'Sǐ',\n",
       "  '-1:word.start_with_capital': 1,\n",
       "  '-1:len(word-1)': 3,\n",
       "  '-1:word.lower()': 'sǐ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 0,\n",
       "  '-1:word.prefix': 'Si',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': '.',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': '.',\n",
       "  '+1:word.tag': 'PUNCT',\n",
       "  '+1:word.position': 10,\n",
       "  '+1:word.letters': '.',\n",
       "  '+1:word.normalized': '.',\n",
       "  '+1:word.start_with_capital': 0,\n",
       "  '+1:len(word+1)': 1,\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.isdigit()': 0,\n",
       "  '+1:word.ispunctuation': 1,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 1},\n",
       " {'word': '.',\n",
       "  'bias': 1.0,\n",
       "  'word.tones': '',\n",
       "  'word.normalized': '.',\n",
       "  'word.position': 10,\n",
       "  'word.has_hyphen': 0,\n",
       "  'word.lower()': '.',\n",
       "  'word.start_with_capital': 0,\n",
       "  'word.have_tone': 0,\n",
       "  'word.prefix': '',\n",
       "  'word.root': '',\n",
       "  'word.ispunctuation': 1,\n",
       "  'word.letters': '.',\n",
       "  'word.isdigit()': 0,\n",
       "  'word.EOS': 1,\n",
       "  'word.BOS': 0,\n",
       "  '-1:word': 'kùʼ',\n",
       "  '-1:word.position': 9,\n",
       "  '-1:word.tag': 'VERB',\n",
       "  '-1:word.letters': 'k u ̀ ʼ',\n",
       "  '-1:word.normalized': 'kùʼ',\n",
       "  '-1:word.start_with_capital': 0,\n",
       "  '-1:len(word-1)': 4,\n",
       "  '-1:word.lower()': 'kùʼ',\n",
       "  '-1:word.isdigit()': 0,\n",
       "  '-1:word.ispunctuation': 0,\n",
       "  '-1:word.BOS': 0,\n",
       "  '-1:word.EOS': 1,\n",
       "  '-1:word.prefix': '',\n",
       "  '-1:word.root': '',\n",
       "  '+1:word.prefix': '',\n",
       "  '+1:word.root': '',\n",
       "  '+1:word': '',\n",
       "  '+1:word.tag': '',\n",
       "  '+1:word.position': 11,\n",
       "  '+1:word.letters': -1,\n",
       "  '+1:word.normalized': '',\n",
       "  '+1:word.start_with_capital': -1,\n",
       "  '+1:len(word+1)': -1,\n",
       "  '+1:word.lower()': '',\n",
       "  '+1:word.isdigit()': -1,\n",
       "  '+1:word.ispunctuation': -1,\n",
       "  '+1:word.BOS': 0,\n",
       "  '+1:word.EOS': 1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_estimator = SangkakPosFeaturisation()\n",
    "feature_estimator.fit([])\n",
    "\n",
    "Xtrain = feature_estimator.transform(list_train_data)\n",
    "Xdev  = feature_estimator.transform(list_dev_data)\n",
    "Xtest = feature_estimator.transform(list_test_data)\n",
    "\n",
    "ytrain = feature_estimator.transform(list_train_data, label=True)\n",
    "ydev   = feature_estimator.transform(list_dev_data, label=True)\n",
    "ytest  = feature_estimator.transform(list_test_data, label=True)\n",
    "\n",
    "Xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Building sagemaker data for classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev] Building sagemaker data for classification\n",
      "[test] Building sagemaker data for classification\n"
     ]
    }
   ],
   "source": [
    "features_types = {\n",
    "    \"categorical_features\": [\n",
    "        'word',\n",
    "        'word.tones',\n",
    "        'word.normalized',\n",
    "        'word.lower()',\n",
    "        'word.prefix',\n",
    "        'word.root',\n",
    "        '-1:word',\n",
    "        '-1:word.tag',\n",
    "        '-1:word.normalized',\n",
    "        '-1:word.lower()',\n",
    "        '-1:word.prefix',\n",
    "        '-1:word.root',\n",
    "        '+1:word.prefix',\n",
    "        '+1:word.root',\n",
    "        '+1:word',\n",
    "        '+1:word.lower()',\n",
    "        '+1:word.tag',\n",
    "        '+1:word.normalized'\n",
    "    ],\n",
    "    \"numerical_features\": [\n",
    "        'bias',\n",
    "        'word.position',\n",
    "        'word.have_tone',\n",
    "        'word.ispunctuation',\n",
    "        'word.isdigit()',\n",
    "        'word.EOS',\n",
    "        'word.BOS',\n",
    "        'word.start_with_capital',\n",
    "        'word.has_hyphen',\n",
    "        '-1:word.position',\n",
    "        '-1:word.start_with_capital',\n",
    "        '-1:len(word-1)',\n",
    "        '+1:word.position',\n",
    "        '+1:word.start_with_capital',\n",
    "        '+1:len(word+1)',\n",
    "        '+1:word.isdigit()',\n",
    "        '+1:word.ispunctuation',\n",
    "        '+1:word.BOS',\n",
    "        '+1:word.EOS',\n",
    "        '-1:word.isdigit()',\n",
    "        '-1:word.ispunctuation',\n",
    "        '-1:word.BOS',\n",
    "        '-1:word.EOS'\n",
    "    ]\n",
    "}\n",
    "\n",
    "xgb_df_train = feature_estimator.transform_to_sagemaker_format(\n",
    "    Xtrain, ytrain,\n",
    "    normalize=features_types\n",
    ")\n",
    "xgb_df_dev = feature_estimator.transform_to_sagemaker_format(\n",
    "    Xdev, ydev, \n",
    "    label='dev',\n",
    "    normalize=features_types\n",
    ")\n",
    "xgb_df_test = feature_estimator.transform_to_sagemaker_format(\n",
    "    Xtest, ytest, \n",
    "    label='test',\n",
    "    normalize=features_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>word</th>\n",
       "      <th>bias</th>\n",
       "      <th>word.tones</th>\n",
       "      <th>word.normalized</th>\n",
       "      <th>word.position</th>\n",
       "      <th>word.has_hyphen</th>\n",
       "      <th>word.lower()</th>\n",
       "      <th>word.start_with_capital</th>\n",
       "      <th>word.have_tone</th>\n",
       "      <th>...</th>\n",
       "      <th>+1:word.position</th>\n",
       "      <th>+1:word.letters</th>\n",
       "      <th>+1:word.normalized</th>\n",
       "      <th>+1:word.start_with_capital</th>\n",
       "      <th>+1:len(word+1)</th>\n",
       "      <th>+1:word.lower()</th>\n",
       "      <th>+1:word.isdigit()</th>\n",
       "      <th>+1:word.ispunctuation</th>\n",
       "      <th>+1:word.BOS</th>\n",
       "      <th>+1:word.EOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>Mwɔ̌ʼ</td>\n",
       "      <td>1</td>\n",
       "      <td>̌</td>\n",
       "      <td>Mwɔ̌ʼ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mwɔ̌ʼ</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>p f ʉ ́ t ə ́</td>\n",
       "      <td>pfʉ́tə́</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>pfʉ́tə́</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VERB</td>\n",
       "      <td>pfʉ́tə́</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>pfʉ́tə́</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pfʉ́tə́</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>n ə ́</td>\n",
       "      <td>nə́</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>nə́</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADP</td>\n",
       "      <td>nə́</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>nə́</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>nə́</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>m w a ̂ s i</td>\n",
       "      <td>mwâsi</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>mwâsi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>mwâsi</td>\n",
       "      <td>1</td>\n",
       "      <td>̂</td>\n",
       "      <td>mwâsi</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>mwâsi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>m a ́ p</td>\n",
       "      <td>máp</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>máp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DET</td>\n",
       "      <td>máp</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>máp</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>máp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>y ə ́</td>\n",
       "      <td>yə́</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>yə́</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24104</th>\n",
       "      <td>VERB</td>\n",
       "      <td>pfʉ́</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>pfʉ́</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>pfʉ́</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>n ə ̂</td>\n",
       "      <td>nə̂</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>nə̂</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24105</th>\n",
       "      <td>ADP</td>\n",
       "      <td>nə̂</td>\n",
       "      <td>1</td>\n",
       "      <td>̂</td>\n",
       "      <td>nə̂</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>nə̂</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>ŋ k a ʼ</td>\n",
       "      <td>ŋkaʼ</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>ŋkaʼ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24106</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>ŋkaʼ</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>ŋkaʼ</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>ŋkaʼ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>8 4</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24107</th>\n",
       "      <td>NUM</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>84</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24108</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24109 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels     word  bias word.tones word.normalized  word.position  \\\n",
       "0       NOUN    Mwɔ̌ʼ     1          ̌           Mwɔ̌ʼ              0   \n",
       "1       VERB  pfʉ́tə́     1                    pfʉ́tə́              1   \n",
       "2        ADP      nə́     1                        nə́              2   \n",
       "3       NOUN   mwâsi     1          ̂          mwâsi              3   \n",
       "4        DET     máp     1                       máp              4   \n",
       "...      ...      ...   ...        ...             ...            ...   \n",
       "24104   VERB     pfʉ́     1                       pfʉ́             11   \n",
       "24105    ADP      nə̂     1          ̂             nə̂             12   \n",
       "24106   NOUN     ŋkaʼ     1                       ŋkaʼ             13   \n",
       "24107    NUM       84     1                         84             14   \n",
       "24108  PUNCT        .     1                          .             15   \n",
       "\n",
       "       word.has_hyphen word.lower()  word.start_with_capital  word.have_tone  \\\n",
       "0                    0        mwɔ̌ʼ                       -1               1   \n",
       "1                    0      pfʉ́tə́                        0               1   \n",
       "2                    0          nə́                        0               1   \n",
       "3                    0       mwâsi                        0               1   \n",
       "4                    0         máp                        0               1   \n",
       "...                ...          ...                      ...             ...   \n",
       "24104                0         pfʉ́                        0               1   \n",
       "24105                0          nə̂                        0               1   \n",
       "24106                0         ŋkaʼ                        0               1   \n",
       "24107                0           84                        0               1   \n",
       "24108                0            .                        0               0   \n",
       "\n",
       "       ... +1:word.position +1:word.letters  +1:word.normalized  \\\n",
       "0      ...                1   p f ʉ ́ t ə ́             pfʉ́tə́   \n",
       "1      ...                2           n ə ́                 nə́   \n",
       "2      ...                3     m w a ̂ s i              mwâsi   \n",
       "3      ...                4         m a ́ p                máp   \n",
       "4      ...                5           y ə ́                 yə́   \n",
       "...    ...              ...             ...                 ...   \n",
       "24104  ...               12           n ə ̂                 nə̂   \n",
       "24105  ...               13         ŋ k a ʼ                ŋkaʼ   \n",
       "24106  ...               14             8 4                  84   \n",
       "24107  ...               15               .                   .   \n",
       "24108  ...               16              -1                       \n",
       "\n",
       "      +1:word.start_with_capital  +1:len(word+1)  +1:word.lower()  \\\n",
       "0                              0               7          pfʉ́tə́   \n",
       "1                              0               3              nə́   \n",
       "2                              0               6           mwâsi   \n",
       "3                              0               4             máp   \n",
       "4                              0               3              yə́   \n",
       "...                          ...             ...              ...   \n",
       "24104                          0               3              nə̂   \n",
       "24105                          0               4             ŋkaʼ   \n",
       "24106                          0               2               84   \n",
       "24107                          0               1                .   \n",
       "24108                         -1              -1                    \n",
       "\n",
       "       +1:word.isdigit() +1:word.ispunctuation  +1:word.BOS +1:word.EOS  \n",
       "0                      0                     0            0           0  \n",
       "1                      0                     0            0           0  \n",
       "2                      0                     0            0           0  \n",
       "3                      0                     0            0           0  \n",
       "4                      0                     0            0           0  \n",
       "...                  ...                   ...          ...         ...  \n",
       "24104                  0                     0            0           0  \n",
       "24105                  0                     0            0           0  \n",
       "24106                  1                     0            0           0  \n",
       "24107                  0                     1            0           1  \n",
       "24108                 -1                    -1            0           1  \n",
       "\n",
       "[24109 rows x 45 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([xgb_df_train, xgb_df_dev, xgb_df_test], \n",
    "                     axis=0, ignore_index=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mystring = \"Welcome\"\n",
    "mybytes = mystring.encode('utf-8')\n",
    "myint = int.from_bytes(mybytes, 'little')\n",
    "print(myint)\n",
    "recoveredbytes = myint.to_bytes((myint.bit_length() + 7) // 8, 'little')\n",
    "recoveredstring = recoveredbytes.decode('utf-8')\n",
    "print(recoveredstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels                        int64\n",
       "word                          int64\n",
       "bias                          int64\n",
       "word.tones                    int64\n",
       "word.normalized               int64\n",
       "word.position                 int64\n",
       "word.has_hyphen               int64\n",
       "word.lower()                  int64\n",
       "word.start_with_capital       int64\n",
       "word.have_tone                int64\n",
       "word.prefix                   int64\n",
       "word.root                     int64\n",
       "word.ispunctuation            int64\n",
       "word.letters                  int64\n",
       "word.isdigit()                int64\n",
       "word.EOS                      int64\n",
       "word.BOS                      int64\n",
       "-1:word                       int64\n",
       "-1:word.position              int64\n",
       "-1:word.tag                   int64\n",
       "-1:word.letters               int64\n",
       "-1:word.normalized            int64\n",
       "-1:word.start_with_capital    int64\n",
       "-1:len(word-1)                int64\n",
       "-1:word.lower()               int64\n",
       "-1:word.isdigit()             int64\n",
       "-1:word.ispunctuation         int64\n",
       "-1:word.BOS                   int64\n",
       "-1:word.EOS                   int64\n",
       "-1:word.prefix                int64\n",
       "-1:word.root                  int64\n",
       "+1:word.prefix                int64\n",
       "+1:word.root                  int64\n",
       "+1:word                       int64\n",
       "+1:word.tag                   int64\n",
       "+1:word.position              int64\n",
       "+1:word.letters               int64\n",
       "+1:word.normalized            int64\n",
       "+1:word.start_with_capital    int64\n",
       "+1:len(word+1)                int64\n",
       "+1:word.lower()               int64\n",
       "+1:word.isdigit()             int64\n",
       "+1:word.ispunctuation         int64\n",
       "+1:word.BOS                   int64\n",
       "+1:word.EOS                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def string_int_transform(r):\n",
    "    mybytes = r.encode('utf-8')\n",
    "    myint = int.from_bytes(mybytes, 'little')\n",
    "    return myint\n",
    "\n",
    "all_ref_data, all_labels = {}, {}\n",
    "\n",
    "def apply_ref_transform(r):\n",
    "    global all_ref_data\n",
    "    if r not in all_ref_data: \n",
    "        ref = random.randint(100000, 7000000)\n",
    "        all_ref_data[r] = ref\n",
    "        return ref\n",
    "    else: return all_ref_data[r]\n",
    "\n",
    "def apply_label_transform(r):\n",
    "    global all_labels\n",
    "    if r not in all_labels: \n",
    "        ref = random.randint(0, 15)\n",
    "        all_labels[r] = ref\n",
    "        return ref\n",
    "    else: return all_labels[r]\n",
    "\n",
    "i = 0\n",
    "for x in all_data['labels']:\n",
    "    if x not in all_labels:\n",
    "        all_labels[x] = i\n",
    "        i += 1\n",
    "\n",
    "all_data_parse = all_data.copy()\n",
    "\n",
    "all_data_parse['labels'] = all_data['labels'].map(all_labels).astype(\"int\")\n",
    "all_data_parse['word'] = all_data['word'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['word.tones'] = all_data['word.tones'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['word.normalized'] = all_data['word.normalized'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['word.lower()'] = all_data['word.lower()'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['word.prefix'] = all_data['word.prefix'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['word.root'] = all_data['word.root'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['-1:word'] = all_data['-1:word'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['-1:word.tag'] = all_data['-1:word.tag'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['-1:word.normalized'] = all_data['-1:word.normalized'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['-1:word.lower()'] = all_data['-1:word.lower()'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['+1:word'] = all_data['+1:word'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['+1:word.lower()'] = all_data['+1:word.lower()'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['+1:word.tag'] = all_data['+1:word.tag'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['+1:word.normalized'] = all_data['+1:word.normalized'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['+1:word.prefix'] = all_data['+1:word.prefix'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['+1:word.root'] = all_data['+1:word.root'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['-1:word.prefix'] = all_data['-1:word.prefix'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['-1:word.root'] = all_data['-1:word.root'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['+1:word.letters'] = all_data['+1:word.letters'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['-1:word.letters'] = all_data['-1:word.letters'].apply(apply_ref_transform).astype(\"int\")\n",
    "all_data_parse['word.letters'] = all_data['word.letters'].apply(apply_ref_transform).astype(\"int\")\n",
    "\n",
    "# all_data_parse = all_data_parse.astype(int)\n",
    "\n",
    "all_data_parse.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unused / non performants varaibles\n",
    "for x in ['+1:word.isdigit()', '+1:word.ispunctuation', '-1:word.EOS',\n",
    "        '+1:word.BOS', 'word.has_hyphen', '+1:word.EOS', '-1:word.BOS',\n",
    "        '+1:word.EOS', '-1:word.isdigit()', '+1:word.BOS', \n",
    "        '-1:word.ispunctuation', '-1:word.BOS', '+1:word.normalized',\n",
    "        '-1:word.EOS', '-1:word.tag', '+1:word.tag', 'word.EOS',\n",
    "        '-1:word.start_with_capital','+1:word.start_with_capital']:\n",
    "    try: del all_data_parse[x]\n",
    "    except: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dython.nominal import associations\n",
    "\n",
    "# check correlation between all variables in dataset\n",
    "associations(all_data_parse, nom_nom_assoc='theil', figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 16\n",
      "Type of target of ytrain data set: multiclass\n",
      "Type of target of ytest data set: multiclass\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "xgb_X_train, xgb_X_test, xgb_y_train, xgb_y_test = train_test_split(\n",
    "    all_data_parse.drop('labels', axis=1).copy(),\n",
    "    all_data_parse['labels'].copy(),\n",
    "    test_size=0.2, random_state=None, shuffle=False\n",
    ")\n",
    "\n",
    "xgb_X_train, xgb_X_dev, xgb_y_train, xgb_y_dev = train_test_split(\n",
    "    xgb_X_train, xgb_y_train, test_size=0.25, \n",
    "    random_state=None, shuffle=False\n",
    ")\n",
    "\n",
    "num_class = len(list(set(all_data_parse['labels'])))\n",
    "print(\"Number of classes: %s\" %num_class)\n",
    "\n",
    "print(\"Type of target of ytrain data set: %s\" %type_of_target(xgb_y_train))\n",
    "print(\"Type of target of ytest data set: %s\" %type_of_target(xgb_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30781835 0.38178315 1.16203406 ... 0.38178315 0.59360637 1.47964403]\n"
     ]
    }
   ],
   "source": [
    "# Compute weights by class for unbalanced datasets\n",
    "optimised_labels_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=xgb_y_train\n",
    ")\n",
    "\n",
    "print(optimised_labels_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/29 [00:01<00:35,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostClassifier', 'Accuracy': 0.28038158440481126, 'Balanced Accuracy': 0.13333333333333333, 'ROC AUC': None, 'F1 Score': 0.1536212697033047, 'Time taken': 1.2779781818389893}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/29 [00:02<00:38,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BaggingClassifier', 'Accuracy': 0.7287432600580672, 'Balanced Accuracy': 0.6793960115097673, 'ROC AUC': None, 'F1 Score': 0.7285582602599514, 'Time taken': 1.5556800365447998}\n",
      "{'Model': 'BernoulliNB', 'Accuracy': 0.4328079635006222, 'Balanced Accuracy': 0.33185060713680076, 'ROC AUC': None, 'F1 Score': 0.42433378404471517, 'Time taken': 0.048421382904052734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 4/29 [01:00<07:43, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'CalibratedClassifierCV', 'Accuracy': 0.4968892575694733, 'Balanced Accuracy': 0.3610991133777006, 'ROC AUC': None, 'F1 Score': 0.46935457094405014, 'Time taken': 57.36065340042114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 6/29 [01:00<03:47,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DecisionTreeClassifier', 'Accuracy': 0.6688096225632517, 'Balanced Accuracy': 0.6431891390474653, 'ROC AUC': None, 'F1 Score': 0.6713659271000213, 'Time taken': 0.36971354484558105}\n",
      "{'Model': 'DummyClassifier', 'Accuracy': 0.19576939029448362, 'Balanced Accuracy': 0.06666666666666667, 'ROC AUC': None, 'F1 Score': 0.06410208270481876, 'Time taken': 0.0255889892578125}\n",
      "{'Model': 'ExtraTreeClassifier', 'Accuracy': 0.5255080879303193, 'Balanced Accuracy': 0.45971911529415677, 'ROC AUC': None, 'F1 Score': 0.5289519454831997, 'Time taken': 0.042963504791259766}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 9/29 [01:02<01:44,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ExtraTreesClassifier', 'Accuracy': 0.7442969722107009, 'Balanced Accuracy': 0.6459167099754325, 'ROC AUC': None, 'F1 Score': 0.7414449277880977, 'Time taken': 1.7849054336547852}\n",
      "{'Model': 'GaussianNB', 'Accuracy': 0.366860223973455, 'Balanced Accuracy': 0.2998089777009991, 'ROC AUC': None, 'F1 Score': 0.34363498208081006, 'Time taken': 0.04032158851623535}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 11/29 [01:02<01:04,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'KNeighborsClassifier', 'Accuracy': 0.5470759021153049, 'Balanced Accuracy': 0.43356864129990574, 'ROC AUC': None, 'F1 Score': 0.539842972228343, 'Time taken': 0.29000401496887207}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 12/29 [01:11<01:16,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LabelPropagation', 'Accuracy': 0.5176275404396516, 'Balanced Accuracy': 0.43694959737458916, 'ROC AUC': None, 'F1 Score': 0.5203587969926515, 'Time taken': 8.55701470375061}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 13/29 [01:29<01:59,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LabelSpreading', 'Accuracy': 0.5176275404396516, 'Balanced Accuracy': 0.43694959737458916, 'ROC AUC': None, 'F1 Score': 0.5203589275061763, 'Time taken': 18.517716646194458}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 14/29 [01:30<01:27,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LinearDiscriminantAnalysis', 'Accuracy': 0.4962671090833679, 'Balanced Accuracy': 0.36672575728184437, 'ROC AUC': None, 'F1 Score': 0.4732971710532519, 'Time taken': 0.27785515785217285}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 15/29 [01:47<02:01,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LinearSVC', 'Accuracy': 0.49730402322687683, 'Balanced Accuracy': 0.3528560821417585, 'ROC AUC': None, 'F1 Score': 0.45655781936007844, 'Time taken': 17.374536991119385}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 16/29 [01:49<01:29,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LogisticRegression', 'Accuracy': 0.5066362505184571, 'Balanced Accuracy': 0.36792277854140937, 'ROC AUC': None, 'F1 Score': 0.48050521917202466, 'Time taken': 1.704707145690918}\n",
      "{'Model': 'NearestCentroid', 'Accuracy': 0.35918705931148903, 'Balanced Accuracy': 0.33188046863940907, 'ROC AUC': None, 'F1 Score': 0.37165947018311957, 'Time taken': 0.05567455291748047}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 19/29 [01:49<00:33,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'PassiveAggressiveClassifier', 'Accuracy': 0.3531729572791373, 'Balanced Accuracy': 0.2856454897660464, 'ROC AUC': None, 'F1 Score': 0.34310560705608634, 'Time taken': 0.4362154006958008}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 21/29 [01:50<00:17,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'Perceptron', 'Accuracy': 0.3896723351306512, 'Balanced Accuracy': 0.31431560508328504, 'ROC AUC': None, 'F1 Score': 0.3833791404350315, 'Time taken': 0.3060479164123535}\n",
      "{'Model': 'QuadraticDiscriminantAnalysis', 'Accuracy': 0.1897552882621319, 'Balanced Accuracy': 0.20848511411809814, 'ROC AUC': None, 'F1 Score': 0.20889384351694784, 'Time taken': 0.15691184997558594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 22/29 [01:53<00:17,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'RandomForestClassifier', 'Accuracy': 0.7461634176690171, 'Balanced Accuracy': 0.647425353923911, 'ROC AUC': None, 'F1 Score': 0.7433845430608437, 'Time taken': 3.539419651031494}\n",
      "{'Model': 'RidgeClassifier', 'Accuracy': 0.48963085856491084, 'Balanced Accuracy': 0.3332021949432608, 'ROC AUC': None, 'F1 Score': 0.4394841232981389, 'Time taken': 0.06689190864562988}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 24/29 [01:54<00:07,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'RidgeClassifierCV', 'Accuracy': 0.48963085856491084, 'Balanced Accuracy': 0.3332021949432608, 'ROC AUC': None, 'F1 Score': 0.4394841232981389, 'Time taken': 0.23463058471679688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 25/29 [01:55<00:05,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'SGDClassifier', 'Accuracy': 0.42948983824139364, 'Balanced Accuracy': 0.3217481489630157, 'ROC AUC': None, 'F1 Score': 0.4139354071411019, 'Time taken': 1.007359266281128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 26/29 [02:07<00:12,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'SVC', 'Accuracy': 0.5723766072169224, 'Balanced Accuracy': 0.4224295109310907, 'ROC AUC': None, 'F1 Score': 0.5517286448568289, 'Time taken': 12.81466269493103}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [02:27<00:06,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'XGBClassifier', 'Accuracy': 0.7774782248029863, 'Balanced Accuracy': 0.6951193949069833, 'ROC AUC': None, 'F1 Score': 0.7765965425476828, 'Time taken': 19.173996925354004}\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4484\n",
      "[LightGBM] [Info] Number of data points in the train set: 14465, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -1.594343\n",
      "[LightGBM] [Info] Start training from score -1.809686\n",
      "[LightGBM] [Info] Start training from score -2.922761\n",
      "[LightGBM] [Info] Start training from score -2.550400\n",
      "[LightGBM] [Info] Start training from score -2.730421\n",
      "[LightGBM] [Info] Start training from score -2.632511\n",
      "[LightGBM] [Info] Start training from score -3.164390\n",
      "[LightGBM] [Info] Start training from score -2.251050\n",
      "[LightGBM] [Info] Start training from score -2.894875\n",
      "[LightGBM] [Info] Start training from score -2.728302\n",
      "[LightGBM] [Info] Start training from score -3.693383\n",
      "[LightGBM] [Info] Start training from score -4.086426\n",
      "[LightGBM] [Info] Start training from score -3.926998\n",
      "[LightGBM] [Info] Start training from score -3.368887\n",
      "[LightGBM] [Info] Start training from score -8.480875\n",
      "[LightGBM] [Info] Start training from score -8.886340\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:30<00:00,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LGBMClassifier', 'Accuracy': 0.3216507673164662, 'Balanced Accuracy': 0.28325516799755207, 'ROC AUC': None, 'F1 Score': 0.3231265714919173, 'Time taken': 3.284762144088745}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.70</td>\n",
       "      <td>None</td>\n",
       "      <td>0.78</td>\n",
       "      <td>19.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>None</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>None</td>\n",
       "      <td>0.74</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.65</td>\n",
       "      <td>None</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.64</td>\n",
       "      <td>None</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "      <td>None</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>0.52</td>\n",
       "      <td>18.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>0.52</td>\n",
       "      <td>8.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>None</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.42</td>\n",
       "      <td>None</td>\n",
       "      <td>0.55</td>\n",
       "      <td>12.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>None</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.37</td>\n",
       "      <td>None</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>None</td>\n",
       "      <td>0.47</td>\n",
       "      <td>57.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.35</td>\n",
       "      <td>None</td>\n",
       "      <td>0.46</td>\n",
       "      <td>17.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.33</td>\n",
       "      <td>None</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.33</td>\n",
       "      <td>None</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>None</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.33</td>\n",
       "      <td>None</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.32</td>\n",
       "      <td>None</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.31</td>\n",
       "      <td>None</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.30</td>\n",
       "      <td>None</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.29</td>\n",
       "      <td>None</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.28</td>\n",
       "      <td>None</td>\n",
       "      <td>0.32</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>None</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.13</td>\n",
       "      <td>None</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
       "Model                                                                          \n",
       "XGBClassifier                      0.78               0.70    None      0.78   \n",
       "BaggingClassifier                  0.73               0.68    None      0.73   \n",
       "RandomForestClassifier             0.75               0.65    None      0.74   \n",
       "ExtraTreesClassifier               0.74               0.65    None      0.74   \n",
       "DecisionTreeClassifier             0.67               0.64    None      0.67   \n",
       "ExtraTreeClassifier                0.53               0.46    None      0.53   \n",
       "LabelSpreading                     0.52               0.44    None      0.52   \n",
       "LabelPropagation                   0.52               0.44    None      0.52   \n",
       "KNeighborsClassifier               0.55               0.43    None      0.54   \n",
       "SVC                                0.57               0.42    None      0.55   \n",
       "LogisticRegression                 0.51               0.37    None      0.48   \n",
       "LinearDiscriminantAnalysis         0.50               0.37    None      0.47   \n",
       "CalibratedClassifierCV             0.50               0.36    None      0.47   \n",
       "LinearSVC                          0.50               0.35    None      0.46   \n",
       "RidgeClassifier                    0.49               0.33    None      0.44   \n",
       "RidgeClassifierCV                  0.49               0.33    None      0.44   \n",
       "NearestCentroid                    0.36               0.33    None      0.37   \n",
       "BernoulliNB                        0.43               0.33    None      0.42   \n",
       "SGDClassifier                      0.43               0.32    None      0.41   \n",
       "Perceptron                         0.39               0.31    None      0.38   \n",
       "GaussianNB                         0.37               0.30    None      0.34   \n",
       "PassiveAggressiveClassifier        0.35               0.29    None      0.34   \n",
       "LGBMClassifier                     0.32               0.28    None      0.32   \n",
       "QuadraticDiscriminantAnalysis      0.19               0.21    None      0.21   \n",
       "AdaBoostClassifier                 0.28               0.13    None      0.15   \n",
       "DummyClassifier                    0.20               0.07    None      0.06   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "XGBClassifier                       19.17  \n",
       "BaggingClassifier                    1.56  \n",
       "RandomForestClassifier               3.54  \n",
       "ExtraTreesClassifier                 1.78  \n",
       "DecisionTreeClassifier               0.37  \n",
       "ExtraTreeClassifier                  0.04  \n",
       "LabelSpreading                      18.52  \n",
       "LabelPropagation                     8.56  \n",
       "KNeighborsClassifier                 0.29  \n",
       "SVC                                 12.81  \n",
       "LogisticRegression                   1.70  \n",
       "LinearDiscriminantAnalysis           0.28  \n",
       "CalibratedClassifierCV              57.36  \n",
       "LinearSVC                           17.37  \n",
       "RidgeClassifier                      0.07  \n",
       "RidgeClassifierCV                    0.23  \n",
       "NearestCentroid                      0.06  \n",
       "BernoulliNB                          0.05  \n",
       "SGDClassifier                        1.01  \n",
       "Perceptron                           0.31  \n",
       "GaussianNB                           0.04  \n",
       "PassiveAggressiveClassifier          0.44  \n",
       "LGBMClassifier                       3.28  \n",
       "QuadraticDiscriminantAnalysis        0.16  \n",
       "AdaBoostClassifier                   1.28  \n",
       "DummyClassifier                      0.03  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "clf = LazyClassifier(verbose=1,ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(xgb_X_train, xgb_X_test, xgb_y_train, xgb_y_test)\n",
    "models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scia-crf-lf-OXAjte5Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "646b439550bec9cacc5e0384422c9ee78f8df74b182cfe1fc7410e07b34d6961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
