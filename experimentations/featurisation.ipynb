{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Levis0045/SCIA-CRF_LF/blob/0.1/training/experimentations_crf_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sangkak AI Challenge: NER tasks\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "- **Author**: Elvis MBONING (NTeALan Research and Development Team)\n",
        "- **Session**: février 2023\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "In this notebook, we try to implement new methods which can potentialy improved NER task in low african resource languages.\n",
        "\n",
        "We propose a rule-based approach call **Position to position entity augmentation** to normalize and augment lowest training data for CRF model. Our work is based on this paper (Xiang Dai and Heike Adel, 2020)[https://aclanthology.org/2020.coling-main.343.pdf]."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments : features engineering\n",
        "\n",
        "In this experiment, we want to build features for differents algorithms and tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDW7Mxx1-RoP",
        "outputId": "9eac6c2c-417c-4f9a-f347-81eb25017b8a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from pathlib import Path\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "fQbKTIOV-Oq2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Reading folder path\n",
        "\n",
        "date = \"2023-08-21\"\n",
        "folder = \"./preprocessing\"\n",
        "\n",
        "bbj_pos_prep_path = Path(f'{folder}/sangkak_input_df_data_bbj_{date}.joblib')\n",
        "\n",
        "bbj_pos_data = joblib.load(bbj_pos_prep_path) \n",
        "\n",
        "pd_train_data = bbj_pos_data[\"train_data\"]\n",
        "pd_dev_data = bbj_pos_data[\"dev_data\"]\n",
        "pd_test_data = bbj_pos_data[\"test_data\"]\n",
        "extracted_test_data = bbj_pos_data[\"list_test_data\"]\n",
        "extracted_train_data = bbj_pos_data[\"list_train_data\"]\n",
        "extracted_dev_data = bbj_pos_data[\"list_dev_data\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_train_data, pd_dev_data , pd_test_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eu3GNuDX-Oq7"
      },
      "source": [
        "## 1- Features engineering for sklearn algo\n",
        "\n",
        "We will use differents kind of features to modelize our model. As ghomala is an african language, it is important to consider some of its features.\n",
        "\n",
        "Any Bantu or semi-Bantu language use tone markers as morpho-syntatic properties to differentiate word or meaning. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ybFUFTwx-Oq7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Loading dependents libraries\n",
        "\n",
        "import unidecode\n",
        "import re\n",
        "from datetime import datetime\n",
        "import string\n",
        "import math\n",
        "import unicodedata"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Features based on african linguistics specificities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importing features module \n",
        "# from features import number_tone_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ33Z586-Oq7",
        "outputId": "ad2d04d6-27c8-4385-c4c1-a1d459e53e47",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 2 ---̀\n",
            "['n', 't', 'â', 'm', 'g', 'ǒ']\n",
            "['û', 'ꞌ', 'ŋ', 'ə', '̂', 'ô', '̌', 'ʼ', 'ê', 'ï', 'è', 'ɛ', 'ǔ', 'ǝ', 'ǎ', 'ú', 'ó', '̀', 'ì', 'í', '̈', 'ʉ', 'à', 'ǐ', 'î', 'ɔ', 'ǒ', 'ù', 'á', '̩', 'é', 'â', 'ě', '́'] ́ ̄ ̀ ̌ ̂ \n",
            "['û', 'ꞌ', 'ŋ', 'ə', '̂', 'ô', '̌', 'ʼ', 'ê', 'ï', 'è', 'ɛ', 'ǔ', 'ǝ', 'ǎ', 'ú', 'ó', '̀', 'ì', 'í', '̈', 'ʉ', 'à', 'ǐ', 'î', 'ɔ', 'ǒ', 'ù', 'á', '̩', 'é', 'â', 'ě', '́']\n"
          ]
        }
      ],
      "source": [
        "# Constructing word features based on tones and API charaters\n",
        "\n",
        "all_words = list(set(pd_train_data[\"word\"].values))\n",
        "all_tags  = list(set(pd_train_data[\"tags\"].values))\n",
        "\n",
        "words_caracters = set([y.lower() for x in all_words for y in x])\n",
        "all_caracters   = string.punctuation+string.ascii_letters+string.digits+''\n",
        "tone_caracters  = list(set([x for x in words_caracters if x not in all_caracters]))\n",
        "cpm_search      = re.compile(str(tone_caracters))\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    \"\"\"Remove accents from input string in other to get ascii string\n",
        "\n",
        "    Args:\n",
        "        input_str (str): input string\n",
        "\n",
        "    Returns:\n",
        "        str: output ascii string\n",
        "    \"\"\"\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    #print([x for x in nfkd_form if x not in string.ascii_letters])\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode('utf8')\n",
        "\n",
        "def extract_tone(input_str):\n",
        "    \"\"\"Extract tone from input string\n",
        "\n",
        "    Args:\n",
        "        input_str (str): input string\n",
        "\n",
        "    Returns:\n",
        "        str: tones found from input string\n",
        "    \"\"\"\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    #print([x for x in nfkd_form if x not in string.ascii_letters])\n",
        "    tones = [x for x in nfkd_form if x not in bantou_letters]\n",
        "    str_tones =  \"\".join(list(set([x.strip() for x in tones \n",
        "                     if x in \" ̄ ̀ ̌ ̂ '\"])))\n",
        "    # print(str_tones)\n",
        "    return str_tones if len(str_tones) != 0 else None\n",
        "    \n",
        "# Set of functions that normalizes and get features from datasets\n",
        "bantou_tones = [f\"{x} \" for x in \" ́̄̀̌̂\" if x != \" \"]\n",
        "string_tones = \"\".join(bantou_tones)\n",
        "tones_search = re.compile(string_tones)\n",
        "\n",
        "bantou_letters = string.ascii_letters+\"ǝɔᵾɓɨşœɑʉɛɗŋøẅëïə\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "non_tone = remove_accents(\"fə̀fə̀\")\n",
        "\n",
        "print(\n",
        "    len(\"fə̀fə̀\"), \n",
        "    len(non_tone), \n",
        "    \"---\"+extract_tone(\"fə̀fə̀\")\n",
        ")\n",
        "\n",
        "print([x for x in \"ntâmgǒ\"])\n",
        "print(tone_caracters, string_tones)\n",
        "print(tone_caracters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "int(True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Features based on word and its contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "oTLG98FL-Oq8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# l'ajout des tags suivants au mot courant améliore significativement le modèle\n",
        "# l'ajout des informations sur les tons\n",
        "\n",
        "bantou_tones = [f\"{x} \" for x in \" ́̄̀̌̂\" if x != \" \"]\n",
        "string_tones = \"\".join(bantou_tones)\n",
        "tones_search = re.compile(string_tones)\n",
        "\n",
        "def word_decomposition(input_str):\n",
        "    \"\"\"Decompse input string in to words\n",
        "\n",
        "    Args:\n",
        "        input_str (str): input string\n",
        "\n",
        "    Returns:\n",
        "        str: input string\n",
        "    \"\"\"\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    word_decomp = \" \".join([x for x in nfkd_form ])\n",
        "    return word_decomp\n",
        "\n",
        "def number_tone_word(input_str):\n",
        "    \"\"\"Get number of tone found in the input string\n",
        "\n",
        "    Args:\n",
        "        input_str (str): input string\n",
        "\n",
        "    Returns:\n",
        "        int: number of tone found in the input string\n",
        "    \"\"\"\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    tone_str = [x for x in nfkd_form if x not in bantou_letters]\n",
        "\n",
        "    return len([x.strip() for x in tone_str \n",
        "                     if x not in ['.', 'Ŋ', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
        "    \n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    len_tone = number_tone_word(word)\n",
        "    tones = extract_tone(word)\n",
        "    features = {\n",
        "        'word': word,\n",
        "        #'bias': 1.0,\n",
        "        'word.tones': tones if tones else \"\",\n",
        "        'word.normalized': unicodedata.normalize('NFKD', word),\n",
        "        'word.position': i,\n",
        "        'word.has_hyphen': int('-' in word),\n",
        "        'word.lower()': word.lower(),\n",
        "        'word.start_with_capital': int(word[0].isupper()) if i > 0 else -1,\n",
        "        'word.have_tone': 1 if len_tone>0 else 0,\n",
        "        'word.prefix': word[:2] if len(word)>2 else \"\",\n",
        "        'word.root': word[3:] if len(word)>2 else \"\",\n",
        "        'word.ispunctuation': int(word in string.punctuation),\n",
        "        'word.isdigit()': int(word.isdigit()),\n",
        "        'word.EOS': 1 if word in ['.','?','!'] else 0,\n",
        "        'word.BOS': 1 if i == 0 else 0,\n",
        "        '-1:word': sent[i-1][0] if i > 0 else \"\",\n",
        "        '-1:word.position': i-1 if i > 0 else -1,\n",
        "        '-1:word.tag': sent[i-1][1] if i > 0 else \"\",\n",
        "        #'-1:word.letters': word_decomposition(sent[i-1][0]) if i > 0 else -1,\n",
        "        '-1:word.normalized': unicodedata.normalize('NFKD', sent[i-1][0]) if i > 0 else \"\",\n",
        "        '-1:word.start_with_capital': int(sent[i-1][0][0].isupper()) if i > 0 else -1,\n",
        "        '-1:len(word-1)': len(sent[i-1][0]) if i > 0 else -1,\n",
        "        '-1:word.lower()': sent[i-1][0].lower() if i > 0 else \"\",\n",
        "        '-1:word.isdigit()': int(sent[i-1][0].isdigit()) if i > 0 else -1,\n",
        "        '-1:word.ispunctuation': int((sent[i-1][0] in string.punctuation)) if i > 0 else 0,\n",
        "        '-1:word.BOS': 1 if (i-1) == 0 else 0,\n",
        "        '-1:word.EOS': 1 if i > 0 and sent[i-1][0] in ['.','?','!'] else 0,\n",
        "        '+1:word': sent[i+1][0] if i < len(sent)-1 else \"\",\n",
        "        '+1:word.tag': sent[i+1][1] if i < len(sent)-1 else \"\",\n",
        "        '+1:word.position': i+1,\n",
        "        #'+1:word.letters': word_decomposition(sent[i+1][0]) if i < len(sent)-1 else -1,\n",
        "        '+1:word.normalized': unicodedata.normalize('NFKD', sent[i+1][0]) if i < len(sent)-1 else \"\",\n",
        "        '+1:word.start_with_capital': int(sent[i+1][0][0].isupper()) if i < len(sent)-1 else -1,\n",
        "        '+1:len(word+1)': len(sent[i+1][0]) if i < len(sent)-1 else -1,\n",
        "        '+1:word.lower()': sent[i+1][0].lower() if i < len(sent)-1 else \"\",\n",
        "        '+1:word.isdigit()': int(sent[i+1][0].isdigit()) if i < len(sent)-1 else -1,\n",
        "        '+1:word.ispunctuation': int((sent[i+1][0] in string.punctuation)) if i < len(sent)-1 else -1,\n",
        "        '+1:word.BOS': 1 if i < 0 else 0,\n",
        "        '+1:word.EOS': 1 if i < len(sent)-1 and sent[i+1][0] in ['.','?','!'] else 0\n",
        "    }\n",
        "\n",
        "    # if tagword not in ['B-ORG','B-LOC']: features.update({'-1:word.tag()': tagword1})\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [word[1] for word in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [word[0] for word in sent]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Building all features and apply it to all datasets\n",
        "\n",
        "Ces nouveaux estimateurs nous permettent de d'intégrer et de faire converger toute notre chaine de traitement dans un seul bloc de pipeline sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import (\n",
        "    BaseEstimator,\n",
        "    OneToOneFeatureMixin,\n",
        "    TransformerMixin\n",
        ")\n",
        "\n",
        "class SangkakPosFeaturisation(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n",
        "\n",
        "    def __init__(self, norm=\"l2\", *, copy=True):\n",
        "        self.norm = norm\n",
        "        self.copy = copy\n",
        "\n",
        "    def fit(self, X, y=None):  \n",
        "        return self\n",
        "\n",
        "    def transform(self, X, copy=None, label=False):\n",
        "        # check_is_fitted(self)\n",
        "        \n",
        "        copy = copy if copy is not None else self.copy\n",
        "        # X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
        "        train_sents = [[word for word in sentence] for sentence in X]\n",
        "       \n",
        "        if label:\n",
        "            X = [sent2labels(s) for s in train_sents]\n",
        "        else:\n",
        "            X = [sent2features(s) for s in train_sents]\n",
        "        return X\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {\"stateless\": True}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnG_xv72-Oq9",
        "outputId": "763211de-1de3-4c77-946c-d6613448f9c1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Build features from dataset \n",
        "\n",
        "train_sents = [[word for word in sentence] for sentence in extracted_train_data]\n",
        "dev_sents = [[word for word in sentence] for sentence in extracted_dev_data]\n",
        "test_sents = [[word for word in sentence] for sentence in extracted_test_data]\n",
        "\n",
        "\"\"\"\n",
        "print(len(extracted_train_data), len(extracted_test_data))\n",
        "\n",
        "Xtrain = [sent2features(s) for s in train_sents]\n",
        "ytrain = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "Xdev = [sent2features(s) for s in dev_sents]\n",
        "ydev = [sent2labels(s) for s in dev_sents]\n",
        "\n",
        "Xtest = [sent2features(s) for s in test_sents]\n",
        "ytest = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "print(f\"Train X length: {len(Xtrain)} | {len(ytrain)}\")\n",
        "print(f\"Dev X length: {len(Xdev)} | {len(ydev)}\")\n",
        "print(f\"Test X length: {len(Xtest)} | {len(ytest)}\")\n",
        "\n",
        "Xtrain[2]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "posfeat = PosFeaturisation()\n",
        "posfeat.fit([])\n",
        "\n",
        "Xtrain = posfeat.transform(extracted_train_data)\n",
        "Xdev = posfeat.transform(extracted_dev_data)\n",
        "Xtest = posfeat.transform(extracted_test_data)\n",
        "\n",
        "ytrain = posfeat.transform(extracted_train_data, label=True)\n",
        "ydev = posfeat.transform(extracted_dev_data, label=True)\n",
        "ytest = posfeat.transform(extracted_test_data, label=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "WgDr0mtx-Oq9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([{'word': 'Mə́kuʼ',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'Mə́kuʼ',\n",
              "   'word.position': 0,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'mə́kuʼ',\n",
              "   'word.start_with_capital': -1,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'Mə',\n",
              "   'word.root': 'kuʼ',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 1,\n",
              "   '-1:word': '',\n",
              "   '-1:word.position': -1,\n",
              "   '-1:word.tag': '',\n",
              "   '-1:word.normalized': '',\n",
              "   '-1:word.start_with_capital': -1,\n",
              "   '-1:len(word-1)': -1,\n",
              "   '-1:word.lower()': '',\n",
              "   '-1:word.isdigit()': -1,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'dʉmtʉm',\n",
              "   '+1:word.tag': 'ADJ',\n",
              "   '+1:word.position': 1,\n",
              "   '+1:word.normalized': 'dʉmtʉm',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 6,\n",
              "   '+1:word.lower()': 'dʉmtʉm',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'dʉmtʉm',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'dʉmtʉm',\n",
              "   'word.position': 1,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'dʉmtʉm',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': 'dʉ',\n",
              "   'word.root': 'tʉm',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'Mə́kuʼ',\n",
              "   '-1:word.position': 0,\n",
              "   '-1:word.tag': 'NOUN',\n",
              "   '-1:word.normalized': 'Mə́kuʼ',\n",
              "   '-1:word.start_with_capital': 1,\n",
              "   '-1:len(word-1)': 6,\n",
              "   '-1:word.lower()': 'mə́kuʼ',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 1,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'Bi',\n",
              "   '+1:word.tag': 'PROPN',\n",
              "   '+1:word.position': 2,\n",
              "   '+1:word.normalized': 'Bi',\n",
              "   '+1:word.start_with_capital': 1,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'bi',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'Bi',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'Bi',\n",
              "   'word.position': 2,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'bi',\n",
              "   'word.start_with_capital': 1,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'dʉmtʉm',\n",
              "   '-1:word.position': 1,\n",
              "   '-1:word.tag': 'ADJ',\n",
              "   '-1:word.normalized': 'dʉmtʉm',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 6,\n",
              "   '-1:word.lower()': 'dʉmtʉm',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'Mvondo',\n",
              "   '+1:word.tag': 'PROPN',\n",
              "   '+1:word.position': 3,\n",
              "   '+1:word.normalized': 'Mvondo',\n",
              "   '+1:word.start_with_capital': 1,\n",
              "   '+1:len(word+1)': 6,\n",
              "   '+1:word.lower()': 'mvondo',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'Mvondo',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'Mvondo',\n",
              "   'word.position': 3,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'mvondo',\n",
              "   'word.start_with_capital': 1,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': 'Mv',\n",
              "   'word.root': 'ndo',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'Bi',\n",
              "   '-1:word.position': 2,\n",
              "   '-1:word.tag': 'PROPN',\n",
              "   '-1:word.normalized': 'Bi',\n",
              "   '-1:word.start_with_capital': 1,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'bi',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'pú',\n",
              "   '+1:word.tag': 'PRON',\n",
              "   '+1:word.position': 4,\n",
              "   '+1:word.normalized': 'pú',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'pú',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'pú',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'pú',\n",
              "   'word.position': 4,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'pú',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'pu',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'Mvondo',\n",
              "   '-1:word.position': 3,\n",
              "   '-1:word.tag': 'PROPN',\n",
              "   '-1:word.normalized': 'Mvondo',\n",
              "   '-1:word.start_with_capital': 1,\n",
              "   '-1:len(word-1)': 6,\n",
              "   '-1:word.lower()': 'mvondo',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'ké',\n",
              "   '+1:word.tag': 'VERB',\n",
              "   '+1:word.position': 5,\n",
              "   '+1:word.normalized': 'ké',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'ké',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'ké',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'ké',\n",
              "   'word.position': 5,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'ké',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'ke',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'pú',\n",
              "   '-1:word.position': 4,\n",
              "   '-1:word.tag': 'PRON',\n",
              "   '-1:word.normalized': 'pú',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'pú',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'é',\n",
              "   '+1:word.tag': 'PRON',\n",
              "   '+1:word.position': 6,\n",
              "   '+1:word.normalized': 'é',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'é',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'é',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'é',\n",
              "   'word.position': 6,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'é',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'ké',\n",
              "   '-1:word.position': 5,\n",
              "   '-1:word.tag': 'VERB',\n",
              "   '-1:word.normalized': 'ké',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'ké',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'gaə́',\n",
              "   '+1:word.tag': 'SCONJ',\n",
              "   '+1:word.position': 7,\n",
              "   '+1:word.normalized': 'gaə́',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 4,\n",
              "   '+1:word.lower()': 'gaə́',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'gaə́',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'gaə́',\n",
              "   'word.position': 7,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'gaə́',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'ga',\n",
              "   'word.root': '́',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'é',\n",
              "   '-1:word.position': 6,\n",
              "   '-1:word.tag': 'PRON',\n",
              "   '-1:word.normalized': 'é',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'é',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'Stephane',\n",
              "   '+1:word.tag': 'PROPN',\n",
              "   '+1:word.position': 8,\n",
              "   '+1:word.normalized': 'Stephane',\n",
              "   '+1:word.start_with_capital': 1,\n",
              "   '+1:len(word+1)': 8,\n",
              "   '+1:word.lower()': 'stephane',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'Stephane',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'Stephane',\n",
              "   'word.position': 8,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'stephane',\n",
              "   'word.start_with_capital': 1,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': 'St',\n",
              "   'word.root': 'phane',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'gaə́',\n",
              "   '-1:word.position': 7,\n",
              "   '-1:word.tag': 'SCONJ',\n",
              "   '-1:word.normalized': 'gaə́',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 4,\n",
              "   '-1:word.lower()': 'gaə́',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'Nga',\n",
              "   '+1:word.tag': 'PROPN',\n",
              "   '+1:word.position': 9,\n",
              "   '+1:word.normalized': 'Nga',\n",
              "   '+1:word.start_with_capital': 1,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'nga',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'Nga',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'Nga',\n",
              "   'word.position': 9,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'nga',\n",
              "   'word.start_with_capital': 1,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': 'Ng',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'Stephane',\n",
              "   '-1:word.position': 8,\n",
              "   '-1:word.tag': 'PROPN',\n",
              "   '-1:word.normalized': 'Stephane',\n",
              "   '-1:word.start_with_capital': 1,\n",
              "   '-1:len(word-1)': 8,\n",
              "   '-1:word.lower()': 'stephane',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'yə',\n",
              "   '+1:word.tag': 'DET',\n",
              "   '+1:word.position': 10,\n",
              "   '+1:word.normalized': 'yə',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'yə',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'yə',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'yə',\n",
              "   'word.position': 10,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'yə',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'Nga',\n",
              "   '-1:word.position': 9,\n",
              "   '-1:word.tag': 'PROPN',\n",
              "   '-1:word.normalized': 'Nga',\n",
              "   '-1:word.start_with_capital': 1,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'nga',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'e',\n",
              "   '+1:word.tag': 'PRON',\n",
              "   '+1:word.position': 11,\n",
              "   '+1:word.normalized': 'e',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 1,\n",
              "   '+1:word.lower()': 'e',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'e',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'e',\n",
              "   'word.position': 11,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'e',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'yə',\n",
              "   '-1:word.position': 10,\n",
              "   '-1:word.tag': 'DET',\n",
              "   '-1:word.normalized': 'yə',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'yə',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'kə',\n",
              "   '+1:word.tag': 'AUX',\n",
              "   '+1:word.position': 12,\n",
              "   '+1:word.normalized': 'kə',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'kə',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'kə',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'kə',\n",
              "   'word.position': 12,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'kə',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'e',\n",
              "   '-1:word.position': 11,\n",
              "   '-1:word.tag': 'PRON',\n",
              "   '-1:word.normalized': 'e',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 1,\n",
              "   '-1:word.lower()': 'e',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'pfʉ̂',\n",
              "   '+1:word.tag': 'VERB',\n",
              "   '+1:word.position': 13,\n",
              "   '+1:word.normalized': 'pfʉ̂',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 4,\n",
              "   '+1:word.lower()': 'pfʉ̂',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'pfʉ̂',\n",
              "   'word.tones': '̂',\n",
              "   'word.normalized': 'pfʉ̂',\n",
              "   'word.position': 13,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'pfʉ̂',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'pf',\n",
              "   'word.root': '̂',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'kə',\n",
              "   '-1:word.position': 12,\n",
              "   '-1:word.tag': 'AUX',\n",
              "   '-1:word.normalized': 'kə',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'kə',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'bə',\n",
              "   '+1:word.tag': 'SCONJ',\n",
              "   '+1:word.position': 14,\n",
              "   '+1:word.normalized': 'bə',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'bə',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'bə',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'bə',\n",
              "   'word.position': 14,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'bə',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'pfʉ̂',\n",
              "   '-1:word.position': 13,\n",
              "   '-1:word.tag': 'VERB',\n",
              "   '-1:word.normalized': 'pfʉ̂',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 4,\n",
              "   '-1:word.lower()': 'pfʉ̂',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'pú',\n",
              "   '+1:word.tag': 'PRON',\n",
              "   '+1:word.position': 15,\n",
              "   '+1:word.normalized': 'pú',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'pú',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'pú',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'pú',\n",
              "   'word.position': 15,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'pú',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'pu',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'bə',\n",
              "   '-1:word.position': 14,\n",
              "   '-1:word.tag': 'SCONJ',\n",
              "   '-1:word.normalized': 'bə',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'bə',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'cɔp',\n",
              "   '+1:word.tag': 'VERB',\n",
              "   '+1:word.position': 16,\n",
              "   '+1:word.normalized': 'cɔp',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'cɔp',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'cɔp',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'cɔp',\n",
              "   'word.position': 16,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'cɔp',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': 'cɔ',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'pú',\n",
              "   '-1:word.position': 15,\n",
              "   '-1:word.tag': 'PRON',\n",
              "   '-1:word.normalized': 'pú',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'pú',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'é',\n",
              "   '+1:word.tag': 'PRON',\n",
              "   '+1:word.position': 17,\n",
              "   '+1:word.normalized': 'é',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'é',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'é',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'é',\n",
              "   'word.position': 17,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'é',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'cɔp',\n",
              "   '-1:word.position': 16,\n",
              "   '-1:word.tag': 'VERB',\n",
              "   '-1:word.normalized': 'cɔp',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'cɔp',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'á',\n",
              "   '+1:word.tag': 'DET',\n",
              "   '+1:word.position': 18,\n",
              "   '+1:word.normalized': 'á',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'á',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'á',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'á',\n",
              "   'word.position': 18,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'á',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'é',\n",
              "   '-1:word.position': 17,\n",
              "   '-1:word.tag': 'PRON',\n",
              "   '-1:word.normalized': 'é',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'é',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'né',\n",
              "   '+1:word.tag': 'PART',\n",
              "   '+1:word.position': 19,\n",
              "   '+1:word.normalized': 'né',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'né',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'né',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'né',\n",
              "   'word.position': 19,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'né',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'ne',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'á',\n",
              "   '-1:word.position': 18,\n",
              "   '-1:word.tag': 'DET',\n",
              "   '-1:word.normalized': 'á',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'á',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'gɔtí',\n",
              "   '+1:word.tag': 'AUX',\n",
              "   '+1:word.position': 20,\n",
              "   '+1:word.normalized': 'gɔtí',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 5,\n",
              "   '+1:word.lower()': 'gɔtí',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'gɔtí',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'gɔtí',\n",
              "   'word.position': 20,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'gɔtí',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'gɔ',\n",
              "   'word.root': 'í',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'né',\n",
              "   '-1:word.position': 19,\n",
              "   '-1:word.tag': 'PART',\n",
              "   '-1:word.normalized': 'né',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'né',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'ŋwɛ́nyə́',\n",
              "   '+1:word.tag': 'VERB',\n",
              "   '+1:word.position': 21,\n",
              "   '+1:word.normalized': 'ŋwɛ́nyə́',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 8,\n",
              "   '+1:word.lower()': 'ŋwɛ́nyə́',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'ŋwɛ́nyə́',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'ŋwɛ́nyə́',\n",
              "   'word.position': 21,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'ŋwɛ́nyə́',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'ŋw',\n",
              "   'word.root': '́nyə́',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'gɔtí',\n",
              "   '-1:word.position': 20,\n",
              "   '-1:word.tag': 'AUX',\n",
              "   '-1:word.normalized': 'gɔtí',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 5,\n",
              "   '-1:word.lower()': 'gɔtí',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'nə́',\n",
              "   '+1:word.tag': 'PART',\n",
              "   '+1:word.position': 22,\n",
              "   '+1:word.normalized': 'nə́',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'nə́',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'nə́',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'nə́',\n",
              "   'word.position': 22,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'nə́',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'nə',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'ŋwɛ́nyə́',\n",
              "   '-1:word.position': 21,\n",
              "   '-1:word.tag': 'VERB',\n",
              "   '-1:word.normalized': 'ŋwɛ́nyə́',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 8,\n",
              "   '-1:word.lower()': 'ŋwɛ́nyə́',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'ghɔm',\n",
              "   '+1:word.tag': 'VERB',\n",
              "   '+1:word.position': 23,\n",
              "   '+1:word.normalized': 'ghɔm',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 4,\n",
              "   '+1:word.lower()': 'ghɔm',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'ghɔm',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'ghɔm',\n",
              "   'word.position': 23,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'ghɔm',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': 'gh',\n",
              "   'word.root': 'm',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'nə́',\n",
              "   '-1:word.position': 22,\n",
              "   '-1:word.tag': 'PART',\n",
              "   '-1:word.normalized': 'nə́',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'nə́',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'sɔʼpəpúŋ',\n",
              "   '+1:word.tag': 'NOUN',\n",
              "   '+1:word.position': 24,\n",
              "   '+1:word.normalized': 'sɔʼpəpúŋ',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 9,\n",
              "   '+1:word.lower()': 'sɔʼpəpúŋ',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'sɔʼpəpúŋ',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'sɔʼpəpúŋ',\n",
              "   'word.position': 24,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'sɔʼpəpúŋ',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'sɔ',\n",
              "   'word.root': 'pəpúŋ',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'ghɔm',\n",
              "   '-1:word.position': 23,\n",
              "   '-1:word.tag': 'VERB',\n",
              "   '-1:word.normalized': 'ghɔm',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 4,\n",
              "   '-1:word.lower()': 'ghɔm',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'bí',\n",
              "   '+1:word.tag': 'ADP',\n",
              "   '+1:word.position': 25,\n",
              "   '+1:word.normalized': 'bí',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 3,\n",
              "   '+1:word.lower()': 'bí',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'bí',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'bí',\n",
              "   'word.position': 25,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'bí',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'bi',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'sɔʼpəpúŋ',\n",
              "   '-1:word.position': 24,\n",
              "   '-1:word.tag': 'NOUN',\n",
              "   '-1:word.normalized': 'sɔʼpəpúŋ',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 9,\n",
              "   '-1:word.lower()': 'sɔʼpəpúŋ',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'í',\n",
              "   '+1:word.tag': 'PRON',\n",
              "   '+1:word.position': 26,\n",
              "   '+1:word.normalized': 'í',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 2,\n",
              "   '+1:word.lower()': 'í',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'í',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': 'í',\n",
              "   'word.position': 26,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'í',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'bí',\n",
              "   '-1:word.position': 25,\n",
              "   '-1:word.tag': 'ADP',\n",
              "   '-1:word.normalized': 'bí',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 3,\n",
              "   '-1:word.lower()': 'bí',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': 'kɛ́bə̀ŋ',\n",
              "   '+1:word.tag': 'NOUN',\n",
              "   '+1:word.position': 27,\n",
              "   '+1:word.normalized': 'kɛ́bə̀ŋ',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 7,\n",
              "   '+1:word.lower()': 'kɛ́bə̀ŋ',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 0,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0},\n",
              "  {'word': 'kɛ́bə̀ŋ',\n",
              "   'word.tones': '̀',\n",
              "   'word.normalized': 'kɛ́bə̀ŋ',\n",
              "   'word.position': 27,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': 'kɛ́bə̀ŋ',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 1,\n",
              "   'word.prefix': 'kɛ',\n",
              "   'word.root': 'bə̀ŋ',\n",
              "   'word.ispunctuation': 0,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 0,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'í',\n",
              "   '-1:word.position': 26,\n",
              "   '-1:word.tag': 'PRON',\n",
              "   '-1:word.normalized': 'í',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 2,\n",
              "   '-1:word.lower()': 'í',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': '.',\n",
              "   '+1:word.tag': 'PUNCT',\n",
              "   '+1:word.position': 28,\n",
              "   '+1:word.normalized': '.',\n",
              "   '+1:word.start_with_capital': 0,\n",
              "   '+1:len(word+1)': 1,\n",
              "   '+1:word.lower()': '.',\n",
              "   '+1:word.isdigit()': 0,\n",
              "   '+1:word.ispunctuation': 1,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 1},\n",
              "  {'word': '.',\n",
              "   'word.tones': '',\n",
              "   'word.normalized': '.',\n",
              "   'word.position': 28,\n",
              "   'word.has_hyphen': 0,\n",
              "   'word.lower()': '.',\n",
              "   'word.start_with_capital': 0,\n",
              "   'word.have_tone': 0,\n",
              "   'word.prefix': '',\n",
              "   'word.root': '',\n",
              "   'word.ispunctuation': 1,\n",
              "   'word.isdigit()': 0,\n",
              "   'word.EOS': 1,\n",
              "   'word.BOS': 0,\n",
              "   '-1:word': 'kɛ́bə̀ŋ',\n",
              "   '-1:word.position': 27,\n",
              "   '-1:word.tag': 'NOUN',\n",
              "   '-1:word.normalized': 'kɛ́bə̀ŋ',\n",
              "   '-1:word.start_with_capital': 0,\n",
              "   '-1:len(word-1)': 7,\n",
              "   '-1:word.lower()': 'kɛ́bə̀ŋ',\n",
              "   '-1:word.isdigit()': 0,\n",
              "   '-1:word.ispunctuation': 0,\n",
              "   '-1:word.BOS': 0,\n",
              "   '-1:word.EOS': 0,\n",
              "   '+1:word': '',\n",
              "   '+1:word.tag': '',\n",
              "   '+1:word.position': 29,\n",
              "   '+1:word.normalized': '',\n",
              "   '+1:word.start_with_capital': -1,\n",
              "   '+1:len(word+1)': -1,\n",
              "   '+1:word.lower()': '',\n",
              "   '+1:word.isdigit()': -1,\n",
              "   '+1:word.ispunctuation': -1,\n",
              "   '+1:word.BOS': 0,\n",
              "   '+1:word.EOS': 0}],\n",
              " ['NOUN',\n",
              "  'ADJ',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'PRON',\n",
              "  'VERB',\n",
              "  'PRON',\n",
              "  'SCONJ',\n",
              "  'PROPN',\n",
              "  'PROPN',\n",
              "  'DET',\n",
              "  'PRON',\n",
              "  'AUX',\n",
              "  'VERB',\n",
              "  'SCONJ',\n",
              "  'PRON',\n",
              "  'VERB',\n",
              "  'PRON',\n",
              "  'DET',\n",
              "  'PART',\n",
              "  'AUX',\n",
              "  'VERB',\n",
              "  'PART',\n",
              "  'VERB',\n",
              "  'NOUN',\n",
              "  'ADP',\n",
              "  'PRON',\n",
              "  'NOUN',\n",
              "  'PUNCT'])"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Xtrain[2], ytrain[2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['preprocessing/sangkak_featurised_sklearn_train_dev_test_data_2023-08-21.joblib']"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "build_date = str(datetime.now()).split(' ')[0]\n",
        "\n",
        "joblib.dump({\n",
        "    \"Xtrain\": Xtrain, \"ytrain\": ytrain, \n",
        "    \"Xdev\": Xdev, \"ydev\": ydev,\n",
        "    \"Xtest\": Xtest, \"ydev\": ytest\n",
        "}, f'preprocessing/sangkak_featurised_sklearn_train_dev_test_data_{build_date}.joblib') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build data format for sagemaker app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install sagemaker -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le format sagemaker permet de normaliser le nombre de features pour chaque classe observée, contrairement à la stratégie de création de features non uniforme sur chaque classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_sagemaker_classification_format(Xt, yt, label=\"train\"):\n",
        "    print(f\"[{label}] Building sagemaker data for classification\")\n",
        "    columns = ['labels']\n",
        "    columns = columns + list(Xt[0][0].keys())\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "    i = 0\n",
        "    for x, y in zip(Xt, yt):\n",
        "        for v, k in zip(x, y):\n",
        "            row = [k]\n",
        "            row = row + list(v.values())\n",
        "            df.loc[i] = row\n",
        "            i += 1\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[train] Building sagemaker data for classification\n",
            "[dev] Building sagemaker data for classification\n",
            "[test] Building sagemaker data for classification\n"
          ]
        }
      ],
      "source": [
        "df_sg_train = build_sagemaker_classification_format(\n",
        "    Xtrain, ytrain\n",
        ")\n",
        "df_sg_dev = build_sagemaker_classification_format(\n",
        "    Xdev, ydev, \"dev\"\n",
        ")\n",
        "df_sg_test = build_sagemaker_classification_format(\n",
        "    Xtest, ytest, \"test\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>word</th>\n",
              "      <th>word.tones</th>\n",
              "      <th>word.normalized</th>\n",
              "      <th>word.position</th>\n",
              "      <th>word.has_hyphen</th>\n",
              "      <th>word.lower()</th>\n",
              "      <th>word.start_with_capital</th>\n",
              "      <th>word.have_tone</th>\n",
              "      <th>word.prefix</th>\n",
              "      <th>...</th>\n",
              "      <th>+1:word.tag</th>\n",
              "      <th>+1:word.position</th>\n",
              "      <th>+1:word.normalized</th>\n",
              "      <th>+1:word.start_with_capital</th>\n",
              "      <th>+1:len(word+1)</th>\n",
              "      <th>+1:word.lower()</th>\n",
              "      <th>+1:word.isdigit()</th>\n",
              "      <th>+1:word.ispunctuation</th>\n",
              "      <th>+1:word.BOS</th>\n",
              "      <th>+1:word.EOS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NOUN</td>\n",
              "      <td>Mwɔ̌ʼ</td>\n",
              "      <td>̌</td>\n",
              "      <td>Mwɔ̌ʼ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>mwɔ̌ʼ</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>Mw</td>\n",
              "      <td>...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>1</td>\n",
              "      <td>pfʉ́tə́</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>pfʉ́tə́</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VERB</td>\n",
              "      <td>pfʉ́tə́</td>\n",
              "      <td></td>\n",
              "      <td>pfʉ́tə́</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>pfʉ́tə́</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>pf</td>\n",
              "      <td>...</td>\n",
              "      <td>ADP</td>\n",
              "      <td>2</td>\n",
              "      <td>nə́</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>nə́</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADP</td>\n",
              "      <td>nə́</td>\n",
              "      <td></td>\n",
              "      <td>nə́</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>nə́</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>nə</td>\n",
              "      <td>...</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>3</td>\n",
              "      <td>mwâsi</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>mwâsi</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NOUN</td>\n",
              "      <td>mwâsi</td>\n",
              "      <td>̂</td>\n",
              "      <td>mwâsi</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>mwâsi</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>mw</td>\n",
              "      <td>...</td>\n",
              "      <td>DET</td>\n",
              "      <td>4</td>\n",
              "      <td>máp</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>máp</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DET</td>\n",
              "      <td>máp</td>\n",
              "      <td></td>\n",
              "      <td>máp</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>máp</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>ma</td>\n",
              "      <td>...</td>\n",
              "      <td>DET</td>\n",
              "      <td>5</td>\n",
              "      <td>yə́</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>yə́</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12342</th>\n",
              "      <td>AUX</td>\n",
              "      <td>kə</td>\n",
              "      <td></td>\n",
              "      <td>kə</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>kə</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>VERB</td>\n",
              "      <td>7</td>\n",
              "      <td>fǎʼ</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>fǎʼ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12343</th>\n",
              "      <td>VERB</td>\n",
              "      <td>fǎʼ</td>\n",
              "      <td>̌</td>\n",
              "      <td>fǎʼ</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>fǎʼ</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>fa</td>\n",
              "      <td>...</td>\n",
              "      <td>ADP</td>\n",
              "      <td>8</td>\n",
              "      <td>nə́</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>nə́</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12344</th>\n",
              "      <td>ADP</td>\n",
              "      <td>nə́</td>\n",
              "      <td></td>\n",
              "      <td>nə́</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>nə́</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>nə</td>\n",
              "      <td>...</td>\n",
              "      <td>PRON</td>\n",
              "      <td>9</td>\n",
              "      <td>é</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>é</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12345</th>\n",
              "      <td>PRON</td>\n",
              "      <td>é</td>\n",
              "      <td></td>\n",
              "      <td>é</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>é</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>10</td>\n",
              "      <td>.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12346</th>\n",
              "      <td>PUNCT</td>\n",
              "      <td>.</td>\n",
              "      <td></td>\n",
              "      <td>.</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td>11</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12347 rows × 37 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      labels     word word.tones word.normalized  word.position  \\\n",
              "0       NOUN    Mwɔ̌ʼ          ̌           Mwɔ̌ʼ              0   \n",
              "1       VERB  pfʉ́tə́                    pfʉ́tə́              1   \n",
              "2        ADP      nə́                        nə́              2   \n",
              "3       NOUN   mwâsi          ̂          mwâsi              3   \n",
              "4        DET     máp                       máp              4   \n",
              "...      ...      ...        ...             ...            ...   \n",
              "12342    AUX       kə                         kə              6   \n",
              "12343   VERB     fǎʼ          ̌            fǎʼ              7   \n",
              "12344    ADP      nə́                        nə́              8   \n",
              "12345   PRON       é                         é              9   \n",
              "12346  PUNCT        .                          .             10   \n",
              "\n",
              "       word.has_hyphen word.lower()  word.start_with_capital  word.have_tone  \\\n",
              "0                    0        mwɔ̌ʼ                       -1               1   \n",
              "1                    0      pfʉ́tə́                        0               1   \n",
              "2                    0          nə́                        0               1   \n",
              "3                    0       mwâsi                        0               1   \n",
              "4                    0         máp                        0               1   \n",
              "...                ...          ...                      ...             ...   \n",
              "12342                0           kə                        0               0   \n",
              "12343                0         fǎʼ                        0               1   \n",
              "12344                0          nə́                        0               1   \n",
              "12345                0           é                        0               1   \n",
              "12346                0            .                        0               0   \n",
              "\n",
              "      word.prefix  ... +1:word.tag  +1:word.position  +1:word.normalized  \\\n",
              "0              Mw  ...        VERB                 1             pfʉ́tə́   \n",
              "1              pf  ...         ADP                 2                 nə́   \n",
              "2              nə  ...        NOUN                 3              mwâsi   \n",
              "3              mw  ...         DET                 4                máp   \n",
              "4              ma  ...         DET                 5                 yə́   \n",
              "...           ...  ...         ...               ...                 ...   \n",
              "12342              ...        VERB                 7                fǎʼ   \n",
              "12343          fa  ...         ADP                 8                 nə́   \n",
              "12344          nə  ...        PRON                 9                  é   \n",
              "12345              ...       PUNCT                10                   .   \n",
              "12346              ...                            11                       \n",
              "\n",
              "       +1:word.start_with_capital  +1:len(word+1) +1:word.lower()  \\\n",
              "0                               0               7         pfʉ́tə́   \n",
              "1                               0               3             nə́   \n",
              "2                               0               6          mwâsi   \n",
              "3                               0               4            máp   \n",
              "4                               0               3             yə́   \n",
              "...                           ...             ...             ...   \n",
              "12342                           0               4            fǎʼ   \n",
              "12343                           0               3             nə́   \n",
              "12344                           0               2              é   \n",
              "12345                           0               1               .   \n",
              "12346                          -1              -1                   \n",
              "\n",
              "       +1:word.isdigit() +1:word.ispunctuation +1:word.BOS  +1:word.EOS  \n",
              "0                      0                     0           0            0  \n",
              "1                      0                     0           0            0  \n",
              "2                      0                     0           0            0  \n",
              "3                      0                     0           0            0  \n",
              "4                      0                     0           0            0  \n",
              "...                  ...                   ...         ...          ...  \n",
              "12342                  0                     0           0            0  \n",
              "12343                  0                     0           0            0  \n",
              "12344                  0                     0           0            0  \n",
              "12345                  0                     1           0            1  \n",
              "12346                 -1                    -1           0            0  \n",
              "\n",
              "[12347 rows x 37 columns]"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sg_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_features = [\n",
        "   'word',\n",
        "   'word.tones',\n",
        "   'word.normalized',\n",
        "   'word.lower()',\n",
        "   'word.prefix',\n",
        "   'word.root',\n",
        "   '-1:word',\n",
        "   '-1:word.tag',\n",
        "   '-1:word.normalized',\n",
        "   '-1:word.lower()',\n",
        "   '+1:word',\n",
        "   '+1:word.lower()',\n",
        "   '+1:word.tag',\n",
        "   '+1:word.normalized'\n",
        "]\n",
        "\n",
        "numeric_features = [\n",
        "   'word.position',\n",
        "   '-1:word.position',\n",
        "   '-1:word.start_with_capital',\n",
        "   '-1:len(word-1)',\n",
        "   '+1:word.position',\n",
        "   'word.start_with_capital',\n",
        "   'word.has_hyphen',\n",
        "   '+1:word.start_with_capital',\n",
        "   '+1:len(word+1)',\n",
        "   '+1:word.isdigit()',\n",
        "   '+1:word.ispunctuation',\n",
        "   '+1:word.BOS',\n",
        "   '+1:word.EOS',\n",
        "   '-1:word.isdigit()',\n",
        "   '-1:word.ispunctuation',\n",
        "   '-1:word.BOS',\n",
        "   '-1:word.EOS',\n",
        "   'word.have_tone',\n",
        "   'word.ispunctuation',\n",
        "   'word.isdigit()',\n",
        "   'word.EOS',\n",
        "   'word.BOS'\n",
        "]\n",
        "\n",
        "df_sg_train[categorical_features] = df_sg_train[categorical_features].astype(\"category\")\n",
        "assert len(list(df_sg_train.select_dtypes(include=\"category\").columns)) == len(categorical_features)\n",
        "\n",
        "df_sg_train[numeric_features] = df_sg_train[numeric_features].astype(\"int32\")\n",
        "assert len(list(df_sg_train.select_dtypes(include=\"number\").columns)) == len(numeric_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['preprocessing/sangkak_featurised_sagemaker_train_dev_data_2023-08-19.joblib']"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df_sg_train.to_csv(\n",
        "    f'preprocessing/sangkak_featurised_sagemaker_train_data_{build_date}.csv',\n",
        "    encoding='utf-8'\n",
        ")\n",
        "df_sg_dev.to_csv(\n",
        "    f'preprocessing/sangkak_featurised_sagemaker_dev_data_{build_date}.csv',\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "joblib.dump({\n",
        "    \"train\": df_sg_train, \"dev\": df_sg_dev\n",
        "}, f'preprocessing/sangkak_featurised_sagemaker_train_dev_data_{build_date}.joblib') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4- One hot encoding for all features of data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "\n",
        "one_hot_encoder = make_column_transformer(\n",
        "    (\n",
        "        OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
        "        make_column_selector(dtype_include=\"category\"),\n",
        "    ),\n",
        "    remainder=\"passthrough\",\n",
        ")\n",
        "\n",
        "ordinal_encoder = make_column_transformer(\n",
        "    (\n",
        "        StandardScaler(),\n",
        "        make_column_selector(dtype_include=\"number\"),\n",
        "    ),\n",
        "    remainder=\"passthrough\",\n",
        "    # Use short feature names to make it easier to specify the categorical\n",
        "    # variables in the HistGradientBoostingRegressor in the next step\n",
        "    # of the pipeline.\n",
        "    verbose_feature_names_out=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[152], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m train_pca \u001b[39m=\u001b[39m one_hot_encoded_train[one_hot_encoded_train\u001b[39m.\u001b[39mcolumns[\u001b[39m1\u001b[39m:]]\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m X_batch \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marray_split(train_pca, n_batches):\n\u001b[0;32m---> 10\u001b[0m     inc_pca\u001b[39m.\u001b[39;49mpartial_fit(X_batch)\n\u001b[1;32m     12\u001b[0m X_train_reduced \u001b[39m=\u001b[39m inc_pca\u001b[39m.\u001b[39mtransform(train_pca)\n\u001b[1;32m     13\u001b[0m X_train_reduced\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages/sklearn/decomposition/_incremental_pca.py:349\u001b[0m, in \u001b[0;36mIncrementalPCA.partial_fit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    338\u001b[0m     mean_correction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(\n\u001b[1;32m    339\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_seen_ \u001b[39m/\u001b[39m n_total_samples) \u001b[39m*\u001b[39m n_samples\n\u001b[1;32m    340\u001b[0m     ) \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39m-\u001b[39m col_batch_mean)\n\u001b[1;32m    341\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(\n\u001b[1;32m    342\u001b[0m         (\n\u001b[1;32m    343\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingular_values_\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m         )\n\u001b[1;32m    347\u001b[0m     )\n\u001b[0;32m--> 349\u001b[0m U, S, Vt \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39;49msvd(X, full_matrices\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, check_finite\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    350\u001b[0m U, Vt \u001b[39m=\u001b[39m svd_flip(U, Vt, u_based_decision\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    351\u001b[0m explained_variance \u001b[39m=\u001b[39m S\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m/\u001b[39m (n_total_samples \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages/scipy/linalg/_decomp_svd.py:127\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m lwork \u001b[39m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], a1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m    124\u001b[0m                        compute_uv\u001b[39m=\u001b[39mcompute_uv, full_matrices\u001b[39m=\u001b[39mfull_matrices)\n\u001b[1;32m    126\u001b[0m \u001b[39m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m u, s, v, info \u001b[39m=\u001b[39m gesXd(a1, compute_uv\u001b[39m=\u001b[39;49mcompute_uv, lwork\u001b[39m=\u001b[39;49mlwork,\n\u001b[1;32m    128\u001b[0m                       full_matrices\u001b[39m=\u001b[39;49mfull_matrices, overwrite_a\u001b[39m=\u001b[39;49moverwrite_a)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSVD did not converge\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "import numpy as np\n",
        "\n",
        "n_batches = 100\n",
        "\n",
        "inc_pca = IncrementalPCA(n_components=100)\n",
        "\n",
        "train_pca = one_hot_encoded_train[one_hot_encoded_train.columns[1:]]\n",
        "for X_batch in np.array_split(train_pca, n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "        \n",
        "X_train_reduced = inc_pca.transform(train_pca)\n",
        "X_train_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "labels                          object\n",
              "word                          category\n",
              "word.tones                    category\n",
              "word.normalized               category\n",
              "word.position                    int32\n",
              "word.has_hyphen                  int32\n",
              "word.lower()                  category\n",
              "word.start_with_capital          int32\n",
              "word.have_tone                   int32\n",
              "word.prefix                   category\n",
              "word.root                     category\n",
              "word.ispunctuation               int32\n",
              "word.isdigit()                   int32\n",
              "word.EOS                         int32\n",
              "word.BOS                         int32\n",
              "-1:word                       category\n",
              "-1:word.position                 int32\n",
              "-1:word.tag                   category\n",
              "-1:word.normalized            category\n",
              "-1:word.start_with_capital       int32\n",
              "-1:len(word-1)                   int32\n",
              "-1:word.lower()               category\n",
              "-1:word.isdigit()                int32\n",
              "-1:word.ispunctuation            int32\n",
              "-1:word.BOS                      int32\n",
              "-1:word.EOS                      int32\n",
              "+1:word                       category\n",
              "+1:word.tag                   category\n",
              "+1:word.position                 int32\n",
              "+1:word.normalized            category\n",
              "+1:word.start_with_capital       int32\n",
              "+1:len(word+1)                   int32\n",
              "+1:word.lower()               category\n",
              "+1:word.isdigit()                int32\n",
              "+1:word.ispunctuation            int32\n",
              "+1:word.BOS                      int32\n",
              "+1:word.EOS                      int32\n",
              "dtype: object"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sg_train.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'Bunch' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[153], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m pcr \u001b[39m=\u001b[39m make_pipeline(one_hot_encoder, StandardScaler(), IncrementalPCA(n_components\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m))\n\u001b[1;32m     20\u001b[0m pcr\u001b[39m.\u001b[39mfit(train_pca, df_sg_train[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m inc_pca \u001b[39m=\u001b[39m pcr\u001b[39m.\u001b[39;49mnamed_steps(\u001b[39m\"\u001b[39;49m\u001b[39mincrementalpca\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Bunch' object is not callable"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "#n_batches = 100\n",
        "\n",
        "#inc_pca = IncrementalPCA(n_components=100)\n",
        "\n",
        "train_pca = df_sg_train[categorical_features+numeric_features]\n",
        "#for X_batch in np.array_split(train_pca, n_batches):\n",
        "#    inc_pca.partial_fit(X_batch)\n",
        "        \n",
        "pcr = make_pipeline(one_hot_encoder, StandardScaler(), IncrementalPCA(n_components=8))\n",
        "pcr.fit(train_pca, df_sg_train['labels'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Dot product shape mismatch, (12347, 36) vs (25924,)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[182], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m rng \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState(\u001b[39m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m n_samples \u001b[39m=\u001b[39m \u001b[39m12347\u001b[39m\n\u001b[0;32m----> 6\u001b[0m y \u001b[39m=\u001b[39m train_pca\u001b[39m.\u001b[39;49mdot(inc_pca\u001b[39m.\u001b[39;49mcomponents_[\u001b[39m1\u001b[39;49m]) \u001b[39m+\u001b[39m rng\u001b[39m.\u001b[39mnormal(size\u001b[39m=\u001b[39mn_samples) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      8\u001b[0m fig, axes \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m14\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m     10\u001b[0m axes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mscatter(train_pca\u001b[39m.\u001b[39mdot(inc_pca\u001b[39m.\u001b[39mcomponents_[\u001b[39m2\u001b[39m]), y, alpha\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m)\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/scia-crf-lf-OXAjte5Q-py3.10/lib/python3.10/site-packages/pandas/core/frame.py:1603\u001b[0m, in \u001b[0;36mDataFrame.dot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     rvals \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(other)\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m lvals\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m rvals\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m-> 1603\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1604\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDot product shape mismatch, \u001b[39m\u001b[39m{\u001b[39;00mlvals\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m{\u001b[39;00mrvals\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1605\u001b[0m         )\n\u001b[1;32m   1607\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, DataFrame):\n\u001b[1;32m   1608\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(\n\u001b[1;32m   1609\u001b[0m         np\u001b[39m.\u001b[39mdot(lvals, rvals), index\u001b[39m=\u001b[39mleft\u001b[39m.\u001b[39mindex, columns\u001b[39m=\u001b[39mother\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   1610\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Dot product shape mismatch, (12347, 36) vs (25924,)"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "n_samples = 12347\n",
        "\n",
        "y = train_pca.dot(inc_pca.components_[1]) + rng.normal(size=n_samples) / 2\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "axes[0].scatter(train_pca.dot(inc_pca.components_[2]), y, alpha=0.3)\n",
        "axes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\n",
        "\n",
        "axes[1].scatter(train_pca.dot(inc_pca.components_[1]), y, alpha=0.3)\n",
        "axes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hOFiMRD2-Oq9"
      },
      "source": [
        "# 3. Modelling with CRF algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4EOYXRf-Oq9"
      },
      "outputs": [],
      "source": [
        "# import dependents libraries\n",
        "\n",
        "import pycrfsuite\n",
        "#import sklearn_crfsuite\n",
        "import math, string, re\n",
        "import scipy\n",
        "import joblib\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Initialisation of pycrfsuite with training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jN52z5GQQky"
      },
      "outputs": [],
      "source": [
        "# trainer initialisation of pycrfsuite\n",
        "trainer = pycrfsuite.Trainer(verbose=True)\n",
        "\n",
        "for xseq, yseq in zip(Xtrain, ytrain):\n",
        "    trainer.append(xseq, yseq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohqw-e0jSz9l",
        "outputId": "aeeab435-b513-4be6-fbcb-d3486bc804b9"
      },
      "outputs": [],
      "source": [
        "project = \"sangkak-02-2023-aug\"\n",
        "build_date = str(datetime.now()).replace(' ','_')\n",
        "model_name = Path(f\"models/with_aug/crf_pycrfsuite_{project}_{build_date}.model\")\n",
        "model_file = str(model_name)\n",
        "file_crf = Path(f\"models/with_aug/crf_pycrfsuite_{build_date}.object\")\n",
        "\n",
        "print(trainer.params())\n",
        "\n",
        "params = {\n",
        "    #\"algorithm\": 'lbfgs',\n",
        "    \"c1\": 0.0920512484757745,\n",
        "    \"c2\": 0.0328771171605105, \n",
        "    \"max_iterations\":100,\n",
        "    #\"verbose\":True,\n",
        "    \"num_memories\":10000,\n",
        "    \"epsilon\": 1e-3,\n",
        "    \"linesearch\": \"MoreThuente\",\n",
        "    \"max_linesearch\":100000,\n",
        "    \"delta\":1e-4,\n",
        "    #\"n_job\":-1,\n",
        "    #\"c\": 2,\n",
        "    #\"pa_type\": 2,\n",
        "    \"feature.possible_transitions\":True,\n",
        "    \"feature.possible_states\":True, \n",
        "    #\"model_filename\": model_file\n",
        "}\n",
        "\n",
        "trainer.set_params(params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Training and saving pycrfsuite model with training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIj2cHR4RZna"
      },
      "outputs": [],
      "source": [
        "trainer.train(model_file)\n",
        "\n",
        "joblib.dump({\"crf\": trainer, \"params\": params}, file_crf) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.logparser.last_iteration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IVhARMA2-Oq-"
      },
      "source": [
        "# 4. Grid search: find best parameters for our models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JncBFfw4-Oq-"
      },
      "outputs": [],
      "source": [
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "\n",
        "params = {\n",
        "    \"algorithm\": 'lbfgs',\n",
        "    \"max_iterations\":100,\n",
        "    \"verbose\": False,\n",
        "    #\"job\":-1,\n",
        "    \"all_possible_states\":True,\n",
        "    \"all_possible_transitions\":True, \n",
        "    \"model_filename\":model_file\n",
        "}\n",
        "\n",
        "crf_grill = pycrfsuite.Trainer(verbose=True)\n",
        "\n",
        "labels = list(trainer.classes_)\n",
        "labels.remove('O')\n",
        "\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.1),\n",
        "    'c2': scipy.stats.expon(scale=0.05)\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf_grill, \n",
        "                        params_space,\n",
        "                        cv=3,\n",
        "                        verbose=1,\n",
        "                        n_jobs=5,\n",
        "                        n_iter=50,\n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxwJK1dR-Oq-"
      },
      "outputs": [],
      "source": [
        "# crf = rs.best_estimator_\n",
        "print('best params:', rs.best_params_)\n",
        "print('best CV score:', rs.best_score_)\n",
        "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4sNnTjQ-Oq_"
      },
      "outputs": [],
      "source": [
        "#print(rs.cv_results_)\n",
        "_x = [s['c1'] for s in rs.cv_results_['params']]\n",
        "_y = [s['c2'] for s in rs.cv_results_['params']]\n",
        "_c = [s for s in rs.cv_results_['mean_score_time']]\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(12, 12)\n",
        "ax = plt.gca()\n",
        "ax.set_yscale('log')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlabel('C1')\n",
        "ax.set_ylabel('C2')\n",
        "ax.set_title(\"Randomized Hyperparameter Search CV Results (min={:0.3}, max={:0.3})\".format(\n",
        "    min(_c), max(_c)\n",
        "))\n",
        "\n",
        "ax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
        "\n",
        "print(\"Dark blue => {:0.4}, dark red => {:0.4}\".format(min(_c), max(_c)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km6KCWbX-Oq_"
      },
      "outputs": [],
      "source": [
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(Xtest)\n",
        "print(metrics.flat_classification_report(\n",
        "    ytest, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02kA8FA6-Oq_"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
        "\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jajyQFfo-Oq_"
      },
      "outputs": [],
      "source": [
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
        "\n",
        "print(\"Top positive:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common(30))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(crf.state_features_).most_common()[-30:])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "646b439550bec9cacc5e0384422c9ee78f8df74b182cfe1fc7410e07b34d6961"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
